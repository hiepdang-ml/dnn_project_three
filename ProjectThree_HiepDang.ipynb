{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# 1. Build an image segmentation model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42249a",
   "metadata": {},
   "source": [
    "Use CUDA for training and inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112a8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20ca778",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: torch.device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aaa78e",
   "metadata": {},
   "source": [
    "## Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3e554",
   "metadata": {},
   "source": [
    "First, we need to create the `BirdSoundDataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb5a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSoundDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, resolution: Tuple[int, int]):\n",
    "        super().__init__()\n",
    "        self.dataroot: str = dataroot\n",
    "        self.resolution: Tuple[int, int] = resolution\n",
    "        self.image_folder: str = f'{self.dataroot}/images'\n",
    "        self.mask_folder: str = f'{self.dataroot}/masks'\n",
    "        self.image_filenames: List[str] = sorted(os.listdir(self.image_folder))\n",
    "        self.mask_filenames: List[str] = sorted(os.listdir(self.mask_folder))\n",
    "        assert len(self.image_filenames) == len(self.mask_filenames)\n",
    "\n",
    "        self.__transformer = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Grayscale(num_output_channels=1),\n",
    "            T.Resize(size=self.resolution),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image_filename: str = self.image_filenames[idx]\n",
    "        mask_filename: str = self.mask_filenames[idx]\n",
    "        assert image_filename == mask_filename\n",
    "        image_path: str = f'{self.image_folder}/{image_filename}'\n",
    "        mask_path: str = f'{self.mask_folder}/{mask_filename}'\n",
    "        image_tensor: torch.Tensor = self.__transformer(Image.open(image_path))\n",
    "        mask_tensor: torch.Tensor = self.__transformer(Image.open(mask_path))\n",
    "        mask_tensor: torch.Tensor = (mask_tensor != 0).to(dtype=torch.int8)\n",
    "        return image_tensor, mask_tensor\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56104174",
   "metadata": {},
   "source": [
    "Create the train, validation, test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BirdSoundDataset(dataroot='data/train', resolution=(128, 512))\n",
    "val_dataset = BirdSoundDataset(dataroot='data/valid', resolution=(128, 512))\n",
    "test_dataset = BirdSoundDataset(dataroot='data/test', resolution=(128, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012c882",
   "metadata": {},
   "source": [
    "Load the datasets to dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee43cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4a3d7",
   "metadata": {},
   "source": [
    "Let's inspect the data shapes:\n",
    "\n",
    "Training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199c5735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 128, 512])\n",
      "torch.Size([1000, 1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_train_images, sample_train_masks = next(iter(train_dataloader))\n",
    "print(sample_train_images.shape)\n",
    "print(sample_train_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83907764",
   "metadata": {},
   "source": [
    "Validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de25da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 128, 512])\n",
      "torch.Size([200, 1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_val_images, sample_val_masks = next(iter(val_dataloader))\n",
    "print(sample_val_images.shape)\n",
    "print(sample_val_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8b0aa",
   "metadata": {},
   "source": [
    "Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6ddf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 1, 128, 512])\n",
      "torch.Size([300, 1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_test_images, sample_test_masks = next(iter(test_dataloader))\n",
    "print(sample_test_images.shape)\n",
    "print(sample_test_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f6b4b",
   "metadata": {},
   "source": [
    "As can be seen, we treat the input images as grayscale (`n_channels=1`), the output mask has the same resolution with the input images, it also has 1 channel that encodes the groundtruth mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c129bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables to save memory:\n",
    "del sample_train_images, sample_train_masks, sample_val_images, sample_val_masks, sample_test_images, sample_test_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa289df",
   "metadata": {},
   "source": [
    "## Utilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20d3d9",
   "metadata": {},
   "source": [
    "We also need some utility classes to control the training and inferencing process. \n",
    "\n",
    "First, create the `Accumulator` to keep track of the loss and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cd82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from typing import Optional, Dict, TextIO, Any, Tuple, NamedTuple\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import copy\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed45217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"\n",
    "    A utility class for accumulating values for multiple metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.__records: defaultdict[str, float] = defaultdict(float)\n",
    "\n",
    "    def add(self, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Add values to the accumulator.\n",
    "\n",
    "        Parameters:\n",
    "            - **kwargs: named metric and the value is the amount to add.\n",
    "        \"\"\"\n",
    "        metric: str\n",
    "        value: float\n",
    "        for metric, value in kwargs.items():\n",
    "            # Each keyword argument represents a metric name and its value to be added\n",
    "            self.__records[metric] += value\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the accumulator by clearing all recorded metrics.\n",
    "        \"\"\"\n",
    "        self.__records.clear()\n",
    "\n",
    "    def __getitem__(self, key: str) -> float:\n",
    "        \"\"\"\n",
    "        Retrieve a record by key.\n",
    "\n",
    "        Parameters:\n",
    "            - key (str): The record key name.\n",
    "\n",
    "        Returns:\n",
    "            - float: The record value.\n",
    "        \"\"\"\n",
    "        return self.__records[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809156a6",
   "metadata": {},
   "source": [
    "We also need a `EarlyStopping` class to terminate the training process when some evaluation metric stops improving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af269c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    A simple early stopping utility to terminate training when a monitored metric stops improving.\n",
    "\n",
    "    Attributes:\n",
    "        - patience (int): The number of epochs with no improvement after which training will be stopped.\n",
    "        - tolerance (float): The minimum change in the monitored metric to qualify as an improvement,\n",
    "        - considering the direction of the metric being monitored.\n",
    "        - bestscore (float): The best score seen so far.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int, tolerance: float = 0.) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopping instance.\n",
    "        \n",
    "        Parameters:\n",
    "            - patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "            - tolerance (float): The minimum change in the monitored metric to qualify as an improvement. \n",
    "            Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.patience: int = patience\n",
    "        self.tolerance: float = tolerance\n",
    "        self.bestscore: float = float('inf')\n",
    "        self.__counter: int = 0\n",
    "\n",
    "    def __call__(self, value: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the state of the early stopping mechanism based on the new metric value.\n",
    "\n",
    "        Parameters:\n",
    "            - value (float): The latest value of the monitored metric.\n",
    "        \"\"\"\n",
    "        # Improvement or within tolerance, reset counter\n",
    "        if value <= self.bestscore + self.tolerance:\n",
    "            self.bestscore: float = value\n",
    "            self.__counter: int = 0\n",
    "\n",
    "        # No improvement, increment counter\n",
    "        else:\n",
    "            self.__counter += 1\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the training process should be stopped early.\n",
    "\n",
    "        Returns:\n",
    "            - bool: True if training should be stopped (patience exceeded), otherwise False.\n",
    "        \"\"\"\n",
    "        return self.__counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f23ad5",
   "metadata": {},
   "source": [
    "We should also create a `Timer` class to time the training and inferencing process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d1a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "\n",
    "    \"\"\"\n",
    "    A class used to time the duration of epochs and batches.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Timer.\n",
    "        \"\"\"\n",
    "        self.__epoch_starts: Dict[int, float] = dict()\n",
    "        self.__epoch_ends: Dict[int, float] = dict()\n",
    "        self.__batch_starts: Dict[int, Dict[int, float]] = defaultdict(dict)\n",
    "        self.__batch_ends: Dict[int, Dict[int, float]] = defaultdict(dict)\n",
    "\n",
    "    def start_epoch(self, epoch: int) -> None:\n",
    "        \"\"\"\n",
    "        Start timing an epoch.\n",
    "\n",
    "        Parameters:\n",
    "            epoch (int): The epoch number.\n",
    "        \"\"\"\n",
    "        self.__epoch_starts[epoch] = time.time()\n",
    "\n",
    "    def end_epoch(self, epoch: int) -> None:\n",
    "        \"\"\"\n",
    "        End timing an epoch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "        \"\"\"\n",
    "        self.__epoch_ends[epoch] = time.time()\n",
    "\n",
    "    def start_batch(self, epoch: int, batch: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Start timing a batch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "            - batch (int, optional): The batch number. If not provided, the next batch number is used.\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            if self.__batch_starts[epoch]:\n",
    "                batch: int = max(self.__batch_starts[epoch].keys()) + 1\n",
    "            else:\n",
    "                batch: int = 1\n",
    "        self.__batch_starts[epoch][batch] = time.time()\n",
    "    \n",
    "    def end_batch(self, epoch: int, batch: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        End timing a batch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "            - batch (int, optional): The batch number. If not provided, the last started batch number is used.\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            if self.__batch_starts[epoch]:\n",
    "                batch: int = max(self.__batch_starts[epoch].keys())\n",
    "            else:\n",
    "                raise RuntimeError(f\"no batch has started\")\n",
    "        self.__batch_ends[epoch][batch] = time.time()\n",
    "    \n",
    "    def time_epoch(self, epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        Get the duration of an epoch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "\n",
    "        Returns:\n",
    "            - float: The duration of the epoch in seconds.\n",
    "        \"\"\"\n",
    "        result: float = self.__epoch_ends[epoch] - self.__epoch_starts[epoch]\n",
    "        if result > 0:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError(f\"epoch {epoch} ends before starts\")\n",
    "    \n",
    "    def time_batch(self, epoch: int, batch: int) -> float:\n",
    "        \"\"\"\n",
    "        Get the duration of a batch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "            - batch (int): The batch number.\n",
    "\n",
    "        Returns:\n",
    "            - float: The duration of the batch in seconds.\n",
    "        \"\"\"\n",
    "        result: float = self.__batch_ends[epoch][batch] - self.__batch_starts[epoch][batch]\n",
    "        if result > 0:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError(f\"batch {batch} in epoch {epoch} ends before starts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb74c9c",
   "metadata": {},
   "source": [
    "A `Logger` class is implemented to log the messages to the standard output and direct it to a log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838d9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    \"\"\"\n",
    "    A class used to log the training process.\n",
    "\n",
    "    This class provides methods to log messages to a file and the console. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        logfile: str = f\"./.logs/{dt.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    ) -> None:\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialize the logger.\n",
    "\n",
    "        Parameters:\n",
    "            - logfile (str, optional): The path to the logfile. \n",
    "            Defaults to a file in the .logs directory with the current timestamp.\n",
    "        \"\"\"\n",
    "        self.logfile: pathlib.Path = pathlib.Path(logfile)\n",
    "        os.makedirs(name=self.logfile.parent, exist_ok=True)\n",
    "        self._file: TextIO = open(self.logfile, mode='w')\n",
    "\n",
    "    def log(\n",
    "        self, \n",
    "        epoch: int, \n",
    "        n_epochs: int, \n",
    "        batch: Optional[int] = None, \n",
    "        n_batches: Optional[int] = None, \n",
    "        took: Optional[float] = None, \n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log a message to console and a log file\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The current epoch.\n",
    "            - n_epochs (int): The total number of epochs.\n",
    "            - batch (int, optional): The current batch. Defaults to None.\n",
    "            - n_batches (int, optional): The total number of batches. Defaults to None.\n",
    "            - took (float, optional): The time it took to process the batch or epoch. Defaults to None.\n",
    "            - **kwargs: Additional metrics to log.\n",
    "        \"\"\"\n",
    "        suffix: str = ', '.join([f'{metric}: {value:.3e}' for metric, value in kwargs.items()])\n",
    "        prefix: str = f'Epoch {epoch}/{n_epochs} | '\n",
    "        if batch is not None:\n",
    "            prefix += f'Batch {batch}/{n_batches} | '\n",
    "        if took is not None:\n",
    "            prefix += f'Took {took:.2f}s | '\n",
    "        logstring: str = prefix + suffix\n",
    "        print(logstring)\n",
    "        self._file.write(logstring + '\\n')\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the logfile at garbage collected.\n",
    "        \"\"\"\n",
    "        self._file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a6891",
   "metadata": {},
   "source": [
    "The `CheckpointSaver` class is used to regularly save the checkpoints to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0b8683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    \"\"\"\n",
    "    A class used to save PyTorch model and optimizer checkpoints.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        optimizer: Optimizer,\n",
    "        dirpath: str,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CheckPointSaver.\n",
    "\n",
    "        Parameters:\n",
    "            - dirpath (os.PathLike): The directory where the checkpoints to save.\n",
    "            - model (nn.Module): The class object of the model\n",
    "            - optimizer_classname (Optimizer): The class object of the optimizer\n",
    "        \"\"\"\n",
    "        self.dirpath: pathlib.Path = pathlib.Path(dirpath)\n",
    "        # For model reconstruction\n",
    "        self.model_classname: str = model.__class__.__name__\n",
    "        signature: inspect.Signature = inspect.signature(model.__init__)\n",
    "        self.model_kwargs: Dict[str, Any] = {\n",
    "            p: getattr(model, p) for p in signature.parameters.keys() if p != 'self'\n",
    "        }\n",
    "        # For optimizer reconstruction\n",
    "        self.optimizer_classname: str = optimizer.__class__.__name__\n",
    "        # ensure the dirpath exists in the file system\n",
    "        os.makedirs(name=self.dirpath, exist_ok=True)\n",
    "\n",
    "    def save(\n",
    "        self, \n",
    "        model_states: Dict[str, Any],\n",
    "        optimizer_states: Dict[str, Any],\n",
    "        filename: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save checkpoint to a .pt file.\n",
    "\n",
    "        Parameters:\n",
    "            - model_states (Dict[str, torch.Tensor]): The output of model.state_dict()\n",
    "            - optimizer_states (Dict[str, Any]): The output of optimizer.state_dict()\n",
    "            - filename (str): the checkpoint file name\n",
    "        \"\"\"\n",
    "        torch.save(\n",
    "            obj={\n",
    "                'model': {\n",
    "                    'classname' : self.model_classname,\n",
    "                    'kwargs'    : self.model_kwargs,\n",
    "                    'states'    : copy.deepcopy(model_states),\n",
    "                },\n",
    "                'optimizer': {\n",
    "                    'classname' : self.optimizer_classname,\n",
    "                    'states'    : copy.deepcopy(optimizer_states),\n",
    "                }\n",
    "            },\n",
    "            f=os.path.join(self.dirpath, filename)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b4d56",
   "metadata": {},
   "source": [
    "The `CheckpointLoader` class is used to load the checkpoints back to RAM/VRAM for further training or inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff65f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointLoader:\n",
    "    \"\"\"\n",
    "    A class used to load PyTorch model and optimizer checkpoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CheckpointLoader.\n",
    "\n",
    "        Parameters:\n",
    "            - checkpoint_path (str): The path to the checkpoint file.\n",
    "        \"\"\"\n",
    "        self.checkpoint_path: str = checkpoint_path\n",
    "        self.__checkpoint: Dict[str, Any] = torch.load(checkpoint_path, weights_only=False)\n",
    "\n",
    "        # Model metadata\n",
    "        self.model_classname: str = self.__checkpoint['model']['classname']\n",
    "        self.model_kwargs: Dict[str, Any] = self.__checkpoint['model']['kwargs']\n",
    "        \n",
    "        # Optimizer metadata\n",
    "        self.optimizer_classname: str = self.__checkpoint['optimizer']['classname']\n",
    "\n",
    "    def load(self, scope: Dict[str, Any]) -> Tuple[nn.Module, Optimizer]:\n",
    "        \"\"\"\n",
    "        Load the model and optimizer from the checkpoint.\n",
    "\n",
    "        Parameters:\n",
    "            - scope (Dict[str, Any]): The namespace to look up the model and optimizer object. \n",
    "                It's typically the dictionary output of `globals()` or `locals()`\n",
    "        \n",
    "        Returns:\n",
    "            - Tuple[nn.Module, Optimizer]: The model and optimizer loaded from the checkpoint.\n",
    "        \"\"\"\n",
    "        # Check caller's namespace for model object\n",
    "        if self.model_classname not in scope.keys():\n",
    "            raise ImportError(\n",
    "                f'{self.model_classname} is not found in the current namespace, you might need to import it first.'\n",
    "            )\n",
    "        \n",
    "        # Check caller's namespace for optimizer object\n",
    "        if self.optimizer_classname not in scope.keys():\n",
    "            raise ImportError(\n",
    "                f'{self.optimizer_classname} is not found in the current namespace, you might need to import it first.'\n",
    "            )\n",
    "        \n",
    "        # Instantiate model and optimizer\n",
    "        model = eval(self.model_classname, scope)(**self.model_kwargs)\n",
    "        optimizer = eval(self.optimizer_classname, scope)(params=model.parameters())\n",
    "\n",
    "        # Load model from model state_dict and check for compatibility\n",
    "        model_states: Dict[str, Any] = self.__checkpoint['model']['states']\n",
    "        model_incompatible_keys: NamedTuple = model.load_state_dict(model_states)   # inplace update\n",
    "        if model_incompatible_keys.missing_keys:  # List[str]\n",
    "            raise RuntimeError(f'Missing keys from the loaded model checkpoint: {model_incompatible_keys.missing_keys}')\n",
    "        if model_incompatible_keys.unexpected_keys: # List[str]\n",
    "            raise RuntimeError(f'Unexpected keys found in the loaded model checkpoint: {model_incompatible_keys.unexpected_keys}')\n",
    "        \n",
    "        # Load optimizer from optimizer state_dict, it's always compatible\n",
    "        optimizer_states: Dict[str, Any] = self.__checkpoint['optimizer']['states']\n",
    "        optimizer.load_state_dict(optimizer_states) # `load_state_dict` of optimizers always returns None, inplace update\n",
    "\n",
    "        return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f83b7",
   "metadata": {},
   "source": [
    "## Custom Unet Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80f82d",
   "metadata": {},
   "source": [
    "To deal with the image segmentation problem, we can leverage the standard Unet architecture where we can make sure the output's resolution matches the input's resolution. In this research, I added a skip connection in each of the unit block to facilitate smooth backpropagation and avoid vanishing gradient descent.\n",
    "\n",
    "First, we need to define the `UnitBlock`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d794c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e78f8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels: int = in_channels\n",
    "        self.out_channels: int = out_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels, \n",
    "            kernel_size=3, stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels, out_channels=out_channels, \n",
    "            kernel_size=3, stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        self.conv_connection = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels, \n",
    "            kernel_size=1, stride=1, bias=False,\n",
    "        )\n",
    "        self.batchnorm_connection = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        assert self.in_channels == in_channels\n",
    "\n",
    "        identity: torch.Tensor = input\n",
    "\n",
    "        # Local linear transformation\n",
    "        output: torch.Tensor = self.conv1(input)\n",
    "        assert output.shape == (batch_size, self.out_channels, in_height, in_width)\n",
    "        output = self.batchnorm1(output)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        assert output.shape == (batch_size, self.out_channels, in_height, in_width)\n",
    "        output = self.batchnorm2(output)\n",
    "\n",
    "        # Skip Connection\n",
    "        identity = self.conv_connection(identity)\n",
    "        identity = self.batchnorm_connection(identity)\n",
    "\n",
    "        # Merge\n",
    "        assert output.shape == identity.shape\n",
    "        output: torch.Tensor = output + identity\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        assert output.shape == (batch_size, self.out_channels, in_height, in_width)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f79490",
   "metadata": {},
   "source": [
    "Now, we construct the `Encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18f3f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder1 = UnitBlock(in_channels=1, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder2 = UnitBlock(in_channels=64, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder3 = UnitBlock(in_channels=128, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder4 = UnitBlock(in_channels=256, out_channels=512)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Tuple[\n",
    "        torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor\n",
    "    ]:\n",
    "        enc1: torch.Tensor = self.encoder1(input)\n",
    "        enc2: torch.Tensor = self.encoder2(self.pool1(enc1))\n",
    "        enc3: torch.Tensor = self.encoder3(self.pool2(enc2))\n",
    "        enc4: torch.Tensor = self.encoder4(self.pool3(enc3))\n",
    "        return enc1, enc2, enc3, enc4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f5837",
   "metadata": {},
   "source": [
    "And the `BottleNeck`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e6acdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck = UnitBlock(in_channels=512, out_channels=1024)\n",
    "        self.upconv = nn.ConvTranspose2d(\n",
    "            in_channels=1024, out_channels=512, \n",
    "            kernel_size=2, stride=2, bias=False,\n",
    "        )\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4\n",
    "        batch_size, in_channels, height, width = input.shape\n",
    "        assert in_channels == 512\n",
    "        output: torch.Tensor = self.pool(input)\n",
    "        output: torch.Tensor = self.bottleneck(output)\n",
    "        output: torch.Tensor = self.upconv(output)\n",
    "        assert output.shape == (batch_size, 512, height, width)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705460c7",
   "metadata": {},
   "source": [
    "Finally, the `Decoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "247e38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder1 = UnitBlock(in_channels=1024, out_channels=512)\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            in_channels=512, out_channels=256, \n",
    "            kernel_size=2, stride=2, bias=False,\n",
    "        )\n",
    "        self.decoder2 = UnitBlock(in_channels=512, out_channels=256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=256, out_channels=128, \n",
    "            kernel_size=2, stride=2, bias=False,\n",
    "        )\n",
    "        self.decoder3 = UnitBlock(in_channels=256, out_channels=128)\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            in_channels=128, out_channels=64, \n",
    "            kernel_size=2, stride=2, bias=False, \n",
    "        )\n",
    "        self.decoder4 = UnitBlock(in_channels=128, out_channels=64)\n",
    "        self.prediction_head = nn.Conv2d(\n",
    "            in_channels=64, out_channels=1, \n",
    "            kernel_size=1, stride=1, bias=False,\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor, \n",
    "        encoder_outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "    ) -> torch.Tensor:\n",
    "        assert input.ndim == 4\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        assert in_channels == 512\n",
    "        enc1, enc2, enc3, enc4 = encoder_outputs\n",
    "\n",
    "        assert enc4.shape == input.shape\n",
    "        output: torch.Tensor = self.decoder1(torch.cat(tensors=[enc4, input], dim=1))\n",
    "        output: torch.Tensor = self.upconv1(output)\n",
    "\n",
    "        assert enc3.shape == output.shape\n",
    "        output: torch.Tensor = self.decoder2(torch.cat(tensors=[enc3, output], dim=1))\n",
    "        output: torch.Tensor = self.upconv2(output)\n",
    "\n",
    "        assert enc2.shape == output.shape\n",
    "        output: torch.Tensor = self.decoder3(torch.cat(tensors=[enc2, output], dim=1))\n",
    "        output: torch.Tensor = self.upconv3(output)\n",
    "\n",
    "        assert enc1.shape == output.shape\n",
    "        output: torch.Tensor = self.decoder4(torch.cat(tensors=[enc1, output], dim=1))\n",
    "        output: torch.Tensor = self.prediction_head(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cea0d3",
   "metadata": {},
   "source": [
    "The `UNet` model makes use of all the modules above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "368fa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: Encoder, \n",
    "        bottleneck: BottleNeck, \n",
    "        decoder: Decoder = 64, \n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder: Encoder = encoder\n",
    "        self.bottleneck: BottleNeck = bottleneck\n",
    "        self.decoder: Decoder = decoder\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        enc1, enc2, enc3, enc4 = self.encoder(input)\n",
    "        output: torch.Tensor = self.bottleneck(enc4)\n",
    "        output: torch.Tensor = self.decoder(input=output, encoder_outputs=[enc1, enc2, enc3, enc4])\n",
    "        assert output.shape == (batch_size, 1, in_height, in_width)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94186b4a",
   "metadata": {},
   "source": [
    "Let's test the `Unet` model on random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11970f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 128, 512])\n",
      "torch.Size([8, 1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "net = UNet(encoder=Encoder(), bottleneck=BottleNeck(), decoder=Decoder()).to(device=device)\n",
    "x = torch.rand(8, 1, 128, 512, device=device)\n",
    "y = net(x)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a9a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables to save memory:\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba7bc1",
   "metadata": {},
   "source": [
    "The output of the model is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c45b84",
   "metadata": {},
   "source": [
    "# 2. Train your model using [Bird sound datasets](https://yuad-my.sharepoint.com/personal/youshan_zhang_yu_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fyoushan%5Fzhang%5Fyu%5Fedu%2FDocuments%2FBird%5FSound%5FDataset&ga=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a5a26",
   "metadata": {},
   "source": [
    "## Loss function and evaluation metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f38e7b",
   "metadata": {},
   "source": [
    "In order to train the model, we need a to prepare a loss function and a evaluation metric. In this project, I choose the Soft Dice Loss function and the IoU metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50effdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, groundtruths: torch.Tensor) -> torch.Tensor:\n",
    "        assert torch.all((groundtruths == 0) | (groundtruths == 1))\n",
    "        groundtruths: torch.Tensor = groundtruths.float()\n",
    "        probabilities: torch.Tensor = torch.sigmoid(input=logits)\n",
    "        numerator: torch.Tensor = 2. * (probabilities * groundtruths).sum() + 1.\n",
    "        denorminator: torch.Tensor = (probabilities + groundtruths).sum() + 1.\n",
    "        return 1. - numerator / denorminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c49470f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, groundtruths: torch.Tensor) -> torch.Tensor:\n",
    "        assert torch.all((groundtruths == 0) | (groundtruths == 1))\n",
    "        groundtruths: torch.Tensor = groundtruths.int()\n",
    "        probabilities: torch.Tensor = torch.sigmoid(input=logits)\n",
    "        predictions: torch.Tensor = (probabilities > 0.5).int()\n",
    "        intersection: torch.Tensor = predictions & groundtruths\n",
    "        union: torch.Tensor = predictions | groundtruths\n",
    "        return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25177a",
   "metadata": {},
   "source": [
    "We implement the training process in the `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04131c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8f11955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        train_dataset: BirdSoundDataset,\n",
    "        val_dataset: BirdSoundDataset,\n",
    "        train_batch_size: int,\n",
    "        val_batch_size: int,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model: nn.Module = model.to(device=device)\n",
    "        self.optimizer: Optimizer = optimizer\n",
    "        self.train_dataset: BirdSoundDataset = train_dataset\n",
    "        self.val_dataset: BirdSoundDataset = val_dataset\n",
    "        self.train_batch_size: int = train_batch_size\n",
    "        self.val_batch_size: int = val_batch_size\n",
    "        self.device: torch.device = device\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(dataset=val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "        self.loss_function: nn.Module = SoftDiceLoss()\n",
    "        self.evaluation_metric: nn.Module = IOU()\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        n_epochs: int,\n",
    "        patience: int,\n",
    "        tolerance: float,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        save_frequency: int = 5,\n",
    "    ) -> None:\n",
    "        \n",
    "        train_accumulator = Accumulator()\n",
    "        early_stopping = EarlyStopping(patience, tolerance)\n",
    "        timer = Timer()\n",
    "        logger = Logger()\n",
    "        checkpoint_saver = CheckpointSaver(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            dirpath=checkpoint_path,\n",
    "        )\n",
    "        self.model.train()\n",
    "        \n",
    "        # loop through each epoch\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            timer.start_epoch(epoch)\n",
    "            # Loop through each batch\n",
    "            for batch, (batch_image, batch_groundtruth) in enumerate(self.train_dataloader, start=1):\n",
    "                timer.start_batch(epoch, batch)\n",
    "                assert batch_image.ndim == 4\n",
    "                batch_size, n_channels, height, width = batch_image.shape\n",
    "                batch_image: torch.Tensor = batch_image.to(device=self.device)\n",
    "                batch_groundtruth: torch.Tensor = batch_groundtruth.to(device=self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_prediction: torch.Tensor = self.model(input=batch_image)\n",
    "                assert batch_prediction.shape == batch_groundtruth.shape\n",
    "                dice_loss = self.loss_function(\n",
    "                    logits=batch_prediction, groundtruths=batch_groundtruth,\n",
    "                )\n",
    "                dice_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Accumulate the train metrics\n",
    "                with torch.no_grad():\n",
    "                    iou = self.evaluation_metric(\n",
    "                        logits=batch_prediction, groundtruths=batch_groundtruth,\n",
    "                    )\n",
    "\n",
    "                train_accumulator.add(\n",
    "                    dice_loss=dice_loss.item(), \n",
    "                    iou=iou.item(),\n",
    "                )\n",
    "                timer.end_batch(epoch=epoch)\n",
    "                logger.log(\n",
    "                    epoch=epoch, n_epochs=n_epochs, \n",
    "                    batch=batch, n_batches=len(self.train_dataloader), \n",
    "                    took=timer.time_batch(epoch, batch), \n",
    "                    train_dice_loss=train_accumulator['dice_loss'] / batch,  \n",
    "                    train_iou=train_accumulator['iou'] / batch, \n",
    "                )\n",
    "        \n",
    "            # Ragularly save checkpoint\n",
    "            if checkpoint_path is not None and epoch % save_frequency == 0:\n",
    "                checkpoint_saver.save(\n",
    "                    model_states=self.model.state_dict(), \n",
    "                    optimizer_states=self.optimizer.state_dict(),\n",
    "                    filename=f'epoch{epoch}.pt',\n",
    "                )\n",
    "            \n",
    "            # Reset metric records for next epoch\n",
    "            train_accumulator.reset()\n",
    "\n",
    "            # Evaluate\n",
    "            val_dice_loss, val_iou = self.evaluate()\n",
    "            timer.end_epoch(epoch)\n",
    "            logger.log(\n",
    "                epoch=epoch, n_epochs=n_epochs, \n",
    "                took=timer.time_epoch(epoch), \n",
    "                val_dice_loss=val_dice_loss, val_iou=val_iou,\n",
    "            )\n",
    "            print('=' * 20)\n",
    "\n",
    "            early_stopping(val_dice_loss)\n",
    "            if early_stopping:\n",
    "                print('Early Stopped')\n",
    "                break\n",
    "\n",
    "        # Always save last checkpoint\n",
    "        if checkpoint_path:\n",
    "            checkpoint_saver.save(\n",
    "                self.model, \n",
    "                filename=f'epoch{epoch}.pt', \n",
    "                optimizer_states=self.optimizer.state_dict(),\n",
    "            )\n",
    "\n",
    "    def evaluate(self) -> float:\n",
    "        val_accumulator = Accumulator()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Loop through each batch\n",
    "            for batch, (batch_image, batch_groundtruth) in enumerate(self.val_dataloader, start=1):\n",
    "                assert batch_image.ndim == 4\n",
    "                batch_size, n_channels, height, width = batch_image.shape\n",
    "                batch_image: torch.Tensor = batch_image.to(device=self.device)\n",
    "                batch_groundtruth: torch.Tensor = batch_groundtruth.to(device=self.device)\n",
    "                batch_prediction: torch.Tensor = self.model(input=batch_image)\n",
    "                assert batch_prediction.shape == batch_groundtruth.shape\n",
    "                \n",
    "                dice_loss = self.loss_function(\n",
    "                    logits=batch_prediction, groundtruths=batch_groundtruth,\n",
    "                )\n",
    "                iou = self.evaluation_metric(\n",
    "                    logits=batch_prediction, groundtruths=batch_groundtruth,\n",
    "                )\n",
    "                # Accumulate the val metrics\n",
    "                val_accumulator.add(\n",
    "                    val_dice_loss=dice_loss.item(),\n",
    "                    val_iou=iou.item(),\n",
    "                )\n",
    "\n",
    "        # Compute the aggregate metrics\n",
    "        val_dice_loss: float = val_accumulator['val_dice_loss'] / batch\n",
    "        val_iou: float = val_accumulator['val_iou'] / batch\n",
    "        return val_dice_loss, val_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7341988",
   "metadata": {},
   "source": [
    "Now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cb1cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Batch 1/63 | Took 0.77s | train_dice_loss: 8.559e-01, train_iou: 5.983e-02\n",
      "Epoch 1/100 | Batch 2/63 | Took 0.64s | train_dice_loss: 8.313e-01, train_iou: 1.433e-01\n",
      "Epoch 1/100 | Batch 3/63 | Took 0.64s | train_dice_loss: 8.120e-01, train_iou: 1.768e-01\n",
      "Epoch 1/100 | Batch 4/63 | Took 0.64s | train_dice_loss: 7.779e-01, train_iou: 2.116e-01\n",
      "Epoch 1/100 | Batch 5/63 | Took 0.64s | train_dice_loss: 7.475e-01, train_iou: 2.477e-01\n",
      "Epoch 1/100 | Batch 6/63 | Took 0.64s | train_dice_loss: 7.286e-01, train_iou: 2.602e-01\n",
      "Epoch 1/100 | Batch 7/63 | Took 0.64s | train_dice_loss: 7.172e-01, train_iou: 2.671e-01\n",
      "Epoch 1/100 | Batch 8/63 | Took 0.64s | train_dice_loss: 6.958e-01, train_iou: 2.882e-01\n",
      "Epoch 1/100 | Batch 9/63 | Took 0.64s | train_dice_loss: 6.721e-01, train_iou: 3.051e-01\n",
      "Epoch 1/100 | Batch 10/63 | Took 0.64s | train_dice_loss: 6.615e-01, train_iou: 3.141e-01\n",
      "Epoch 1/100 | Batch 11/63 | Took 0.67s | train_dice_loss: 6.504e-01, train_iou: 3.275e-01\n",
      "Epoch 1/100 | Batch 12/63 | Took 0.64s | train_dice_loss: 6.438e-01, train_iou: 3.353e-01\n",
      "Epoch 1/100 | Batch 13/63 | Took 0.71s | train_dice_loss: 6.407e-01, train_iou: 3.370e-01\n",
      "Epoch 1/100 | Batch 14/63 | Took 0.70s | train_dice_loss: 6.357e-01, train_iou: 3.383e-01\n",
      "Epoch 1/100 | Batch 15/63 | Took 0.64s | train_dice_loss: 6.271e-01, train_iou: 3.445e-01\n",
      "Epoch 1/100 | Batch 16/63 | Took 0.64s | train_dice_loss: 6.213e-01, train_iou: 3.487e-01\n",
      "Epoch 1/100 | Batch 17/63 | Took 0.68s | train_dice_loss: 6.152e-01, train_iou: 3.558e-01\n",
      "Epoch 1/100 | Batch 18/63 | Took 0.64s | train_dice_loss: 6.128e-01, train_iou: 3.587e-01\n",
      "Epoch 1/100 | Batch 19/63 | Took 0.64s | train_dice_loss: 6.086e-01, train_iou: 3.629e-01\n",
      "Epoch 1/100 | Batch 20/63 | Took 0.64s | train_dice_loss: 6.056e-01, train_iou: 3.661e-01\n",
      "Epoch 1/100 | Batch 21/63 | Took 0.64s | train_dice_loss: 6.002e-01, train_iou: 3.738e-01\n",
      "Epoch 1/100 | Batch 22/63 | Took 0.64s | train_dice_loss: 5.964e-01, train_iou: 3.764e-01\n",
      "Epoch 1/100 | Batch 23/63 | Took 0.64s | train_dice_loss: 5.931e-01, train_iou: 3.781e-01\n",
      "Epoch 1/100 | Batch 24/63 | Took 0.64s | train_dice_loss: 5.895e-01, train_iou: 3.804e-01\n",
      "Epoch 1/100 | Batch 25/63 | Took 0.64s | train_dice_loss: 5.820e-01, train_iou: 3.862e-01\n",
      "Epoch 1/100 | Batch 26/63 | Took 0.64s | train_dice_loss: 5.768e-01, train_iou: 3.906e-01\n",
      "Epoch 1/100 | Batch 27/63 | Took 0.64s | train_dice_loss: 5.702e-01, train_iou: 3.969e-01\n",
      "Epoch 1/100 | Batch 28/63 | Took 0.64s | train_dice_loss: 5.674e-01, train_iou: 3.987e-01\n",
      "Epoch 1/100 | Batch 29/63 | Took 0.64s | train_dice_loss: 5.632e-01, train_iou: 4.021e-01\n",
      "Epoch 1/100 | Batch 30/63 | Took 0.64s | train_dice_loss: 5.589e-01, train_iou: 4.073e-01\n",
      "Epoch 1/100 | Batch 31/63 | Took 0.64s | train_dice_loss: 5.544e-01, train_iou: 4.126e-01\n",
      "Epoch 1/100 | Batch 32/63 | Took 0.64s | train_dice_loss: 5.509e-01, train_iou: 4.161e-01\n",
      "Epoch 1/100 | Batch 33/63 | Took 0.64s | train_dice_loss: 5.452e-01, train_iou: 4.215e-01\n",
      "Epoch 1/100 | Batch 34/63 | Took 0.64s | train_dice_loss: 5.448e-01, train_iou: 4.208e-01\n",
      "Epoch 1/100 | Batch 35/63 | Took 0.64s | train_dice_loss: 5.442e-01, train_iou: 4.202e-01\n",
      "Epoch 1/100 | Batch 36/63 | Took 0.64s | train_dice_loss: 5.440e-01, train_iou: 4.209e-01\n",
      "Epoch 1/100 | Batch 37/63 | Took 0.64s | train_dice_loss: 5.436e-01, train_iou: 4.219e-01\n",
      "Epoch 1/100 | Batch 38/63 | Took 0.64s | train_dice_loss: 5.437e-01, train_iou: 4.224e-01\n",
      "Epoch 1/100 | Batch 39/63 | Took 0.64s | train_dice_loss: 5.417e-01, train_iou: 4.245e-01\n",
      "Epoch 1/100 | Batch 40/63 | Took 0.64s | train_dice_loss: 5.366e-01, train_iou: 4.296e-01\n",
      "Epoch 1/100 | Batch 41/63 | Took 0.64s | train_dice_loss: 5.348e-01, train_iou: 4.307e-01\n",
      "Epoch 1/100 | Batch 42/63 | Took 0.64s | train_dice_loss: 5.306e-01, train_iou: 4.338e-01\n",
      "Epoch 1/100 | Batch 43/63 | Took 0.64s | train_dice_loss: 5.275e-01, train_iou: 4.359e-01\n",
      "Epoch 1/100 | Batch 44/63 | Took 0.64s | train_dice_loss: 5.264e-01, train_iou: 4.367e-01\n",
      "Epoch 1/100 | Batch 45/63 | Took 0.64s | train_dice_loss: 5.226e-01, train_iou: 4.403e-01\n",
      "Epoch 1/100 | Batch 46/63 | Took 0.64s | train_dice_loss: 5.193e-01, train_iou: 4.428e-01\n",
      "Epoch 1/100 | Batch 47/63 | Took 0.64s | train_dice_loss: 5.170e-01, train_iou: 4.443e-01\n",
      "Epoch 1/100 | Batch 48/63 | Took 0.64s | train_dice_loss: 5.114e-01, train_iou: 4.501e-01\n",
      "Epoch 1/100 | Batch 49/63 | Took 0.64s | train_dice_loss: 5.103e-01, train_iou: 4.500e-01\n",
      "Epoch 1/100 | Batch 50/63 | Took 0.64s | train_dice_loss: 5.081e-01, train_iou: 4.511e-01\n",
      "Epoch 1/100 | Batch 51/63 | Took 0.64s | train_dice_loss: 5.063e-01, train_iou: 4.519e-01\n",
      "Epoch 1/100 | Batch 52/63 | Took 0.64s | train_dice_loss: 5.049e-01, train_iou: 4.524e-01\n",
      "Epoch 1/100 | Batch 53/63 | Took 0.64s | train_dice_loss: 5.034e-01, train_iou: 4.540e-01\n",
      "Epoch 1/100 | Batch 54/63 | Took 0.64s | train_dice_loss: 5.009e-01, train_iou: 4.559e-01\n",
      "Epoch 1/100 | Batch 55/63 | Took 0.64s | train_dice_loss: 4.994e-01, train_iou: 4.564e-01\n",
      "Epoch 1/100 | Batch 56/63 | Took 0.64s | train_dice_loss: 4.960e-01, train_iou: 4.592e-01\n",
      "Epoch 1/100 | Batch 57/63 | Took 0.64s | train_dice_loss: 4.934e-01, train_iou: 4.611e-01\n",
      "Epoch 1/100 | Batch 58/63 | Took 0.64s | train_dice_loss: 4.907e-01, train_iou: 4.629e-01\n",
      "Epoch 1/100 | Batch 59/63 | Took 0.64s | train_dice_loss: 4.877e-01, train_iou: 4.648e-01\n",
      "Epoch 1/100 | Batch 60/63 | Took 0.64s | train_dice_loss: 4.848e-01, train_iou: 4.669e-01\n",
      "Epoch 1/100 | Batch 61/63 | Took 0.64s | train_dice_loss: 4.834e-01, train_iou: 4.671e-01\n",
      "Epoch 1/100 | Batch 62/63 | Took 0.64s | train_dice_loss: 4.811e-01, train_iou: 4.684e-01\n",
      "Epoch 1/100 | Batch 63/63 | Took 0.33s | train_dice_loss: 4.775e-01, train_iou: 4.720e-01\n",
      "Epoch 1/100 | Took 141.82s | val_dice_loss: 5.219e-01, val_iou: 3.689e-01\n",
      "====================\n",
      "Epoch 2/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 4.788e-01, train_iou: 3.913e-01\n",
      "Epoch 2/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 4.969e-01, train_iou: 3.604e-01\n",
      "Epoch 2/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 4.892e-01, train_iou: 3.603e-01\n",
      "Epoch 2/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 4.722e-01, train_iou: 3.728e-01\n",
      "Epoch 2/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 4.705e-01, train_iou: 3.720e-01\n",
      "Epoch 2/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 4.478e-01, train_iou: 3.936e-01\n",
      "Epoch 2/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 4.361e-01, train_iou: 4.049e-01\n",
      "Epoch 2/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 4.313e-01, train_iou: 4.081e-01\n",
      "Epoch 2/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 4.228e-01, train_iou: 4.157e-01\n",
      "Epoch 2/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 4.246e-01, train_iou: 4.131e-01\n",
      "Epoch 2/100 | Batch 11/63 | Took 0.63s | train_dice_loss: 4.164e-01, train_iou: 4.210e-01\n",
      "Epoch 2/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 4.202e-01, train_iou: 4.166e-01\n",
      "Epoch 2/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 4.078e-01, train_iou: 4.298e-01\n",
      "Epoch 2/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 4.042e-01, train_iou: 4.330e-01\n",
      "Epoch 2/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 3.988e-01, train_iou: 4.382e-01\n",
      "Epoch 2/100 | Batch 16/63 | Took 0.62s | train_dice_loss: 3.943e-01, train_iou: 4.426e-01\n",
      "Epoch 2/100 | Batch 17/63 | Took 0.62s | train_dice_loss: 3.871e-01, train_iou: 4.502e-01\n",
      "Epoch 2/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 3.801e-01, train_iou: 4.579e-01\n",
      "Epoch 2/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 3.770e-01, train_iou: 4.607e-01\n",
      "Epoch 2/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 3.807e-01, train_iou: 4.567e-01\n",
      "Epoch 2/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 3.764e-01, train_iou: 4.611e-01\n",
      "Epoch 2/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 3.710e-01, train_iou: 4.670e-01\n",
      "Epoch 2/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 3.769e-01, train_iou: 4.609e-01\n",
      "Epoch 2/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 3.744e-01, train_iou: 4.634e-01\n",
      "Epoch 2/100 | Batch 25/63 | Took 0.62s | train_dice_loss: 3.771e-01, train_iou: 4.603e-01\n",
      "Epoch 2/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 3.755e-01, train_iou: 4.618e-01\n",
      "Epoch 2/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 3.739e-01, train_iou: 4.633e-01\n",
      "Epoch 2/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 3.714e-01, train_iou: 4.658e-01\n",
      "Epoch 2/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 3.717e-01, train_iou: 4.652e-01\n",
      "Epoch 2/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 3.718e-01, train_iou: 4.649e-01\n",
      "Epoch 2/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 3.689e-01, train_iou: 4.680e-01\n",
      "Epoch 2/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 3.670e-01, train_iou: 4.699e-01\n",
      "Epoch 2/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 3.650e-01, train_iou: 4.719e-01\n",
      "Epoch 2/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 3.630e-01, train_iou: 4.739e-01\n",
      "Epoch 2/100 | Batch 35/63 | Took 0.67s | train_dice_loss: 3.629e-01, train_iou: 4.739e-01\n",
      "Epoch 2/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 3.604e-01, train_iou: 4.767e-01\n",
      "Epoch 2/100 | Batch 37/63 | Took 0.62s | train_dice_loss: 3.598e-01, train_iou: 4.772e-01\n",
      "Epoch 2/100 | Batch 38/63 | Took 0.62s | train_dice_loss: 3.589e-01, train_iou: 4.780e-01\n",
      "Epoch 2/100 | Batch 39/63 | Took 0.62s | train_dice_loss: 3.568e-01, train_iou: 4.802e-01\n",
      "Epoch 2/100 | Batch 40/63 | Took 0.62s | train_dice_loss: 3.568e-01, train_iou: 4.800e-01\n",
      "Epoch 2/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 3.561e-01, train_iou: 4.807e-01\n",
      "Epoch 2/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 3.541e-01, train_iou: 4.829e-01\n",
      "Epoch 2/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 3.515e-01, train_iou: 4.859e-01\n",
      "Epoch 2/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 3.502e-01, train_iou: 4.872e-01\n",
      "Epoch 2/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 3.476e-01, train_iou: 4.902e-01\n",
      "Epoch 2/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 3.454e-01, train_iou: 4.928e-01\n",
      "Epoch 2/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 3.440e-01, train_iou: 4.943e-01\n",
      "Epoch 2/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 3.430e-01, train_iou: 4.952e-01\n",
      "Epoch 2/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 3.425e-01, train_iou: 4.957e-01\n",
      "Epoch 2/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 3.404e-01, train_iou: 4.981e-01\n",
      "Epoch 2/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 3.398e-01, train_iou: 4.986e-01\n",
      "Epoch 2/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 3.406e-01, train_iou: 4.977e-01\n",
      "Epoch 2/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 3.382e-01, train_iou: 5.005e-01\n",
      "Epoch 2/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 3.380e-01, train_iou: 5.006e-01\n",
      "Epoch 2/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 3.372e-01, train_iou: 5.014e-01\n",
      "Epoch 2/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 3.382e-01, train_iou: 5.003e-01\n",
      "Epoch 2/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 3.379e-01, train_iou: 5.006e-01\n",
      "Epoch 2/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 3.369e-01, train_iou: 5.016e-01\n",
      "Epoch 2/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 3.366e-01, train_iou: 5.019e-01\n",
      "Epoch 2/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 3.355e-01, train_iou: 5.030e-01\n",
      "Epoch 2/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 3.363e-01, train_iou: 5.021e-01\n",
      "Epoch 2/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 3.359e-01, train_iou: 5.024e-01\n",
      "Epoch 2/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 3.358e-01, train_iou: 5.025e-01\n",
      "Epoch 2/100 | Took 143.73s | val_dice_loss: 3.106e-01, val_iou: 5.403e-01\n",
      "====================\n",
      "Epoch 3/100 | Batch 1/63 | Took 0.64s | train_dice_loss: 3.772e-01, train_iou: 4.525e-01\n",
      "Epoch 3/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 3.136e-01, train_iou: 5.264e-01\n",
      "Epoch 3/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 3.208e-01, train_iou: 5.169e-01\n",
      "Epoch 3/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 3.214e-01, train_iou: 5.156e-01\n",
      "Epoch 3/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 3.020e-01, train_iou: 5.392e-01\n",
      "Epoch 3/100 | Batch 6/63 | Took 0.63s | train_dice_loss: 2.927e-01, train_iou: 5.501e-01\n",
      "Epoch 3/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 2.958e-01, train_iou: 5.461e-01\n",
      "Epoch 3/100 | Batch 8/63 | Took 0.64s | train_dice_loss: 2.965e-01, train_iou: 5.449e-01\n",
      "Epoch 3/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 2.923e-01, train_iou: 5.499e-01\n",
      "Epoch 3/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 2.853e-01, train_iou: 5.585e-01\n",
      "Epoch 3/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 2.821e-01, train_iou: 5.622e-01\n",
      "Epoch 3/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 2.805e-01, train_iou: 5.641e-01\n",
      "Epoch 3/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 2.819e-01, train_iou: 5.622e-01\n",
      "Epoch 3/100 | Batch 14/63 | Took 0.62s | train_dice_loss: 2.785e-01, train_iou: 5.663e-01\n",
      "Epoch 3/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 2.783e-01, train_iou: 5.665e-01\n",
      "Epoch 3/100 | Batch 16/63 | Took 0.62s | train_dice_loss: 2.756e-01, train_iou: 5.698e-01\n",
      "Epoch 3/100 | Batch 17/63 | Took 0.62s | train_dice_loss: 2.788e-01, train_iou: 5.659e-01\n",
      "Epoch 3/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 2.781e-01, train_iou: 5.667e-01\n",
      "Epoch 3/100 | Batch 19/63 | Took 0.62s | train_dice_loss: 2.764e-01, train_iou: 5.688e-01\n",
      "Epoch 3/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 2.842e-01, train_iou: 5.602e-01\n",
      "Epoch 3/100 | Batch 21/63 | Took 0.68s | train_dice_loss: 2.819e-01, train_iou: 5.629e-01\n",
      "Epoch 3/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 2.804e-01, train_iou: 5.647e-01\n",
      "Epoch 3/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 2.810e-01, train_iou: 5.638e-01\n",
      "Epoch 3/100 | Batch 24/63 | Took 0.62s | train_dice_loss: 2.789e-01, train_iou: 5.664e-01\n",
      "Epoch 3/100 | Batch 25/63 | Took 0.62s | train_dice_loss: 2.772e-01, train_iou: 5.685e-01\n",
      "Epoch 3/100 | Batch 26/63 | Took 0.62s | train_dice_loss: 2.749e-01, train_iou: 5.713e-01\n",
      "Epoch 3/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 2.745e-01, train_iou: 5.717e-01\n",
      "Epoch 3/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 2.716e-01, train_iou: 5.756e-01\n",
      "Epoch 3/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 2.717e-01, train_iou: 5.753e-01\n",
      "Epoch 3/100 | Batch 30/63 | Took 0.62s | train_dice_loss: 2.706e-01, train_iou: 5.767e-01\n",
      "Epoch 3/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 2.703e-01, train_iou: 5.769e-01\n",
      "Epoch 3/100 | Batch 32/63 | Took 0.62s | train_dice_loss: 2.707e-01, train_iou: 5.763e-01\n",
      "Epoch 3/100 | Batch 33/63 | Took 0.62s | train_dice_loss: 2.716e-01, train_iou: 5.752e-01\n",
      "Epoch 3/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 2.697e-01, train_iou: 5.776e-01\n",
      "Epoch 3/100 | Batch 35/63 | Took 0.62s | train_dice_loss: 2.700e-01, train_iou: 5.772e-01\n",
      "Epoch 3/100 | Batch 36/63 | Took 0.62s | train_dice_loss: 2.676e-01, train_iou: 5.803e-01\n",
      "Epoch 3/100 | Batch 37/63 | Took 0.62s | train_dice_loss: 2.684e-01, train_iou: 5.793e-01\n",
      "Epoch 3/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 2.668e-01, train_iou: 5.814e-01\n",
      "Epoch 3/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 2.663e-01, train_iou: 5.819e-01\n",
      "Epoch 3/100 | Batch 40/63 | Took 0.62s | train_dice_loss: 2.657e-01, train_iou: 5.827e-01\n",
      "Epoch 3/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 2.697e-01, train_iou: 5.782e-01\n",
      "Epoch 3/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 2.705e-01, train_iou: 5.771e-01\n",
      "Epoch 3/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 2.699e-01, train_iou: 5.779e-01\n",
      "Epoch 3/100 | Batch 44/63 | Took 0.64s | train_dice_loss: 2.715e-01, train_iou: 5.759e-01\n",
      "Epoch 3/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 2.732e-01, train_iou: 5.739e-01\n",
      "Epoch 3/100 | Batch 46/63 | Took 0.62s | train_dice_loss: 2.717e-01, train_iou: 5.758e-01\n",
      "Epoch 3/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 2.714e-01, train_iou: 5.761e-01\n",
      "Epoch 3/100 | Batch 48/63 | Took 0.62s | train_dice_loss: 2.723e-01, train_iou: 5.750e-01\n",
      "Epoch 3/100 | Batch 49/63 | Took 0.62s | train_dice_loss: 2.711e-01, train_iou: 5.765e-01\n",
      "Epoch 3/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 2.715e-01, train_iou: 5.759e-01\n",
      "Epoch 3/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 2.711e-01, train_iou: 5.763e-01\n",
      "Epoch 3/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 2.727e-01, train_iou: 5.745e-01\n",
      "Epoch 3/100 | Batch 53/63 | Took 0.62s | train_dice_loss: 2.727e-01, train_iou: 5.744e-01\n",
      "Epoch 3/100 | Batch 54/63 | Took 0.66s | train_dice_loss: 2.722e-01, train_iou: 5.749e-01\n",
      "Epoch 3/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 2.726e-01, train_iou: 5.744e-01\n",
      "Epoch 3/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 2.718e-01, train_iou: 5.754e-01\n",
      "Epoch 3/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 2.709e-01, train_iou: 5.765e-01\n",
      "Epoch 3/100 | Batch 58/63 | Took 0.62s | train_dice_loss: 2.718e-01, train_iou: 5.754e-01\n",
      "Epoch 3/100 | Batch 59/63 | Took 0.62s | train_dice_loss: 2.711e-01, train_iou: 5.762e-01\n",
      "Epoch 3/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 2.705e-01, train_iou: 5.770e-01\n",
      "Epoch 3/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 2.696e-01, train_iou: 5.781e-01\n",
      "Epoch 3/100 | Batch 62/63 | Took 0.62s | train_dice_loss: 2.694e-01, train_iou: 5.784e-01\n",
      "Epoch 3/100 | Batch 63/63 | Took 0.36s | train_dice_loss: 2.682e-01, train_iou: 5.799e-01\n",
      "Epoch 3/100 | Took 145.58s | val_dice_loss: 2.382e-01, val_iou: 6.248e-01\n",
      "====================\n",
      "Epoch 4/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 2.907e-01, train_iou: 5.494e-01\n",
      "Epoch 4/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 2.659e-01, train_iou: 5.804e-01\n",
      "Epoch 4/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 2.574e-01, train_iou: 5.912e-01\n",
      "Epoch 4/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 2.584e-01, train_iou: 5.899e-01\n",
      "Epoch 4/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 2.663e-01, train_iou: 5.801e-01\n",
      "Epoch 4/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 2.684e-01, train_iou: 5.774e-01\n",
      "Epoch 4/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 2.610e-01, train_iou: 5.870e-01\n",
      "Epoch 4/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 2.569e-01, train_iou: 5.922e-01\n",
      "Epoch 4/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 2.564e-01, train_iou: 5.927e-01\n",
      "Epoch 4/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 2.598e-01, train_iou: 5.885e-01\n",
      "Epoch 4/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 2.550e-01, train_iou: 5.948e-01\n",
      "Epoch 4/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 2.511e-01, train_iou: 5.999e-01\n",
      "Epoch 4/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 2.497e-01, train_iou: 6.016e-01\n",
      "Epoch 4/100 | Batch 14/63 | Took 0.62s | train_dice_loss: 2.525e-01, train_iou: 5.980e-01\n",
      "Epoch 4/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 2.487e-01, train_iou: 6.031e-01\n",
      "Epoch 4/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 2.453e-01, train_iou: 6.075e-01\n",
      "Epoch 4/100 | Batch 17/63 | Took 0.62s | train_dice_loss: 2.475e-01, train_iou: 6.047e-01\n",
      "Epoch 4/100 | Batch 18/63 | Took 0.62s | train_dice_loss: 2.442e-01, train_iou: 6.090e-01\n",
      "Epoch 4/100 | Batch 19/63 | Took 0.62s | train_dice_loss: 2.437e-01, train_iou: 6.096e-01\n",
      "Epoch 4/100 | Batch 20/63 | Took 0.62s | train_dice_loss: 2.453e-01, train_iou: 6.075e-01\n",
      "Epoch 4/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 2.464e-01, train_iou: 6.061e-01\n",
      "Epoch 4/100 | Batch 22/63 | Took 0.62s | train_dice_loss: 2.474e-01, train_iou: 6.047e-01\n",
      "Epoch 4/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 2.438e-01, train_iou: 6.098e-01\n",
      "Epoch 4/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 2.411e-01, train_iou: 6.134e-01\n",
      "Epoch 4/100 | Batch 25/63 | Took 0.62s | train_dice_loss: 2.401e-01, train_iou: 6.146e-01\n",
      "Epoch 4/100 | Batch 26/63 | Took 0.62s | train_dice_loss: 2.394e-01, train_iou: 6.155e-01\n",
      "Epoch 4/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 2.378e-01, train_iou: 6.175e-01\n",
      "Epoch 4/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 2.375e-01, train_iou: 6.179e-01\n",
      "Epoch 4/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 2.385e-01, train_iou: 6.166e-01\n",
      "Epoch 4/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 2.374e-01, train_iou: 6.180e-01\n",
      "Epoch 4/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 2.374e-01, train_iou: 6.180e-01\n",
      "Epoch 4/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 2.366e-01, train_iou: 6.190e-01\n",
      "Epoch 4/100 | Batch 33/63 | Took 0.67s | train_dice_loss: 2.348e-01, train_iou: 6.215e-01\n",
      "Epoch 4/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 2.347e-01, train_iou: 6.215e-01\n",
      "Epoch 4/100 | Batch 35/63 | Took 0.62s | train_dice_loss: 2.350e-01, train_iou: 6.211e-01\n",
      "Epoch 4/100 | Batch 36/63 | Took 0.62s | train_dice_loss: 2.331e-01, train_iou: 6.237e-01\n",
      "Epoch 4/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 2.322e-01, train_iou: 6.248e-01\n",
      "Epoch 4/100 | Batch 38/63 | Took 0.62s | train_dice_loss: 2.333e-01, train_iou: 6.233e-01\n",
      "Epoch 4/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 2.339e-01, train_iou: 6.226e-01\n",
      "Epoch 4/100 | Batch 40/63 | Took 0.62s | train_dice_loss: 2.341e-01, train_iou: 6.224e-01\n",
      "Epoch 4/100 | Batch 41/63 | Took 0.62s | train_dice_loss: 2.324e-01, train_iou: 6.247e-01\n",
      "Epoch 4/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 2.318e-01, train_iou: 6.254e-01\n",
      "Epoch 4/100 | Batch 43/63 | Took 0.62s | train_dice_loss: 2.326e-01, train_iou: 6.243e-01\n",
      "Epoch 4/100 | Batch 44/63 | Took 0.62s | train_dice_loss: 2.322e-01, train_iou: 6.249e-01\n",
      "Epoch 4/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 2.315e-01, train_iou: 6.258e-01\n",
      "Epoch 4/100 | Batch 46/63 | Took 0.62s | train_dice_loss: 2.301e-01, train_iou: 6.277e-01\n",
      "Epoch 4/100 | Batch 47/63 | Took 0.62s | train_dice_loss: 2.323e-01, train_iou: 6.250e-01\n",
      "Epoch 4/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 2.317e-01, train_iou: 6.258e-01\n",
      "Epoch 4/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 2.330e-01, train_iou: 6.241e-01\n",
      "Epoch 4/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 2.327e-01, train_iou: 6.245e-01\n",
      "Epoch 4/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 2.358e-01, train_iou: 6.208e-01\n",
      "Epoch 4/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 2.356e-01, train_iou: 6.210e-01\n",
      "Epoch 4/100 | Batch 53/63 | Took 0.62s | train_dice_loss: 2.347e-01, train_iou: 6.222e-01\n",
      "Epoch 4/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 2.355e-01, train_iou: 6.212e-01\n",
      "Epoch 4/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 2.358e-01, train_iou: 6.207e-01\n",
      "Epoch 4/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 2.349e-01, train_iou: 6.219e-01\n",
      "Epoch 4/100 | Batch 57/63 | Took 0.68s | train_dice_loss: 2.352e-01, train_iou: 6.214e-01\n",
      "Epoch 4/100 | Batch 58/63 | Took 0.62s | train_dice_loss: 2.344e-01, train_iou: 6.225e-01\n",
      "Epoch 4/100 | Batch 59/63 | Took 0.62s | train_dice_loss: 2.339e-01, train_iou: 6.232e-01\n",
      "Epoch 4/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 2.326e-01, train_iou: 6.249e-01\n",
      "Epoch 4/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 2.320e-01, train_iou: 6.257e-01\n",
      "Epoch 4/100 | Batch 62/63 | Took 0.62s | train_dice_loss: 2.340e-01, train_iou: 6.233e-01\n",
      "Epoch 4/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 2.346e-01, train_iou: 6.225e-01\n",
      "Epoch 4/100 | Took 139.43s | val_dice_loss: 2.343e-01, val_iou: 6.309e-01\n",
      "====================\n",
      "Epoch 5/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.457e-01, train_iou: 7.459e-01\n",
      "Epoch 5/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.726e-01, train_iou: 7.067e-01\n",
      "Epoch 5/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.942e-01, train_iou: 6.765e-01\n",
      "Epoch 5/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 2.058e-01, train_iou: 6.605e-01\n",
      "Epoch 5/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.985e-01, train_iou: 6.706e-01\n",
      "Epoch 5/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.992e-01, train_iou: 6.694e-01\n",
      "Epoch 5/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 2.232e-01, train_iou: 6.399e-01\n",
      "Epoch 5/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 2.257e-01, train_iou: 6.360e-01\n",
      "Epoch 5/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 2.219e-01, train_iou: 6.407e-01\n",
      "Epoch 5/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 2.175e-01, train_iou: 6.465e-01\n",
      "Epoch 5/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 2.154e-01, train_iou: 6.491e-01\n",
      "Epoch 5/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 2.134e-01, train_iou: 6.516e-01\n",
      "Epoch 5/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 2.186e-01, train_iou: 6.446e-01\n",
      "Epoch 5/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 2.200e-01, train_iou: 6.425e-01\n",
      "Epoch 5/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 2.209e-01, train_iou: 6.412e-01\n",
      "Epoch 5/100 | Batch 16/63 | Took 0.62s | train_dice_loss: 2.213e-01, train_iou: 6.405e-01\n",
      "Epoch 5/100 | Batch 17/63 | Took 0.62s | train_dice_loss: 2.220e-01, train_iou: 6.394e-01\n",
      "Epoch 5/100 | Batch 18/63 | Took 0.62s | train_dice_loss: 2.233e-01, train_iou: 6.375e-01\n",
      "Epoch 5/100 | Batch 19/63 | Took 0.68s | train_dice_loss: 2.238e-01, train_iou: 6.367e-01\n",
      "Epoch 5/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 2.249e-01, train_iou: 6.352e-01\n",
      "Epoch 5/100 | Batch 21/63 | Took 0.62s | train_dice_loss: 2.243e-01, train_iou: 6.359e-01\n",
      "Epoch 5/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 2.275e-01, train_iou: 6.318e-01\n",
      "Epoch 5/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 2.293e-01, train_iou: 6.293e-01\n",
      "Epoch 5/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 2.299e-01, train_iou: 6.285e-01\n",
      "Epoch 5/100 | Batch 25/63 | Took 0.62s | train_dice_loss: 2.326e-01, train_iou: 6.250e-01\n",
      "Epoch 5/100 | Batch 26/63 | Took 0.64s | train_dice_loss: 2.316e-01, train_iou: 6.262e-01\n",
      "Epoch 5/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 2.330e-01, train_iou: 6.245e-01\n",
      "Epoch 5/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 2.328e-01, train_iou: 6.245e-01\n",
      "Epoch 5/100 | Batch 29/63 | Took 0.62s | train_dice_loss: 2.306e-01, train_iou: 6.275e-01\n",
      "Epoch 5/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 2.295e-01, train_iou: 6.290e-01\n",
      "Epoch 5/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 2.281e-01, train_iou: 6.308e-01\n",
      "Epoch 5/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 2.276e-01, train_iou: 6.314e-01\n",
      "Epoch 5/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 2.258e-01, train_iou: 6.340e-01\n",
      "Epoch 5/100 | Batch 34/63 | Took 0.62s | train_dice_loss: 2.257e-01, train_iou: 6.340e-01\n",
      "Epoch 5/100 | Batch 35/63 | Took 0.62s | train_dice_loss: 2.250e-01, train_iou: 6.349e-01\n",
      "Epoch 5/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 2.279e-01, train_iou: 6.313e-01\n",
      "Epoch 5/100 | Batch 37/63 | Took 0.62s | train_dice_loss: 2.261e-01, train_iou: 6.337e-01\n",
      "Epoch 5/100 | Batch 38/63 | Took 0.62s | train_dice_loss: 2.265e-01, train_iou: 6.331e-01\n",
      "Epoch 5/100 | Batch 39/63 | Took 0.62s | train_dice_loss: 2.265e-01, train_iou: 6.330e-01\n",
      "Epoch 5/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 2.245e-01, train_iou: 6.358e-01\n",
      "Epoch 5/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 2.250e-01, train_iou: 6.352e-01\n",
      "Epoch 5/100 | Batch 42/63 | Took 0.64s | train_dice_loss: 2.239e-01, train_iou: 6.366e-01\n",
      "Epoch 5/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 2.222e-01, train_iou: 6.389e-01\n",
      "Epoch 5/100 | Batch 44/63 | Took 0.62s | train_dice_loss: 2.226e-01, train_iou: 6.384e-01\n",
      "Epoch 5/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 2.220e-01, train_iou: 6.391e-01\n",
      "Epoch 5/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 2.217e-01, train_iou: 6.395e-01\n",
      "Epoch 5/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 2.212e-01, train_iou: 6.401e-01\n",
      "Epoch 5/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 2.208e-01, train_iou: 6.406e-01\n",
      "Epoch 5/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 2.219e-01, train_iou: 6.392e-01\n",
      "Epoch 5/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 2.212e-01, train_iou: 6.401e-01\n",
      "Epoch 5/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 2.206e-01, train_iou: 6.409e-01\n",
      "Epoch 5/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 2.198e-01, train_iou: 6.419e-01\n",
      "Epoch 5/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 2.205e-01, train_iou: 6.410e-01\n",
      "Epoch 5/100 | Batch 54/63 | Took 0.62s | train_dice_loss: 2.203e-01, train_iou: 6.413e-01\n",
      "Epoch 5/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 2.204e-01, train_iou: 6.411e-01\n",
      "Epoch 5/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 2.202e-01, train_iou: 6.412e-01\n",
      "Epoch 5/100 | Batch 57/63 | Took 0.62s | train_dice_loss: 2.198e-01, train_iou: 6.418e-01\n",
      "Epoch 5/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 2.199e-01, train_iou: 6.416e-01\n",
      "Epoch 5/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 2.192e-01, train_iou: 6.425e-01\n",
      "Epoch 5/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 2.192e-01, train_iou: 6.425e-01\n",
      "Epoch 5/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 2.202e-01, train_iou: 6.412e-01\n",
      "Epoch 5/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 2.209e-01, train_iou: 6.403e-01\n",
      "Epoch 5/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 2.198e-01, train_iou: 6.418e-01\n",
      "Epoch 5/100 | Took 139.88s | val_dice_loss: 2.783e-01, val_iou: 5.775e-01\n",
      "====================\n",
      "Epoch 6/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 2.918e-01, train_iou: 5.482e-01\n",
      "Epoch 6/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 2.563e-01, train_iou: 5.932e-01\n",
      "Epoch 6/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 2.271e-01, train_iou: 6.327e-01\n",
      "Epoch 6/100 | Batch 4/63 | Took 0.63s | train_dice_loss: 2.191e-01, train_iou: 6.430e-01\n",
      "Epoch 6/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 2.338e-01, train_iou: 6.240e-01\n",
      "Epoch 6/100 | Batch 6/63 | Took 0.63s | train_dice_loss: 2.430e-01, train_iou: 6.118e-01\n",
      "Epoch 6/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 2.369e-01, train_iou: 6.197e-01\n",
      "Epoch 6/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 2.300e-01, train_iou: 6.287e-01\n",
      "Epoch 6/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 2.343e-01, train_iou: 6.230e-01\n",
      "Epoch 6/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 2.412e-01, train_iou: 6.141e-01\n",
      "Epoch 6/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 2.375e-01, train_iou: 6.189e-01\n",
      "Epoch 6/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 2.361e-01, train_iou: 6.205e-01\n",
      "Epoch 6/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 2.382e-01, train_iou: 6.176e-01\n",
      "Epoch 6/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 2.329e-01, train_iou: 6.248e-01\n",
      "Epoch 6/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 2.357e-01, train_iou: 6.211e-01\n",
      "Epoch 6/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 2.360e-01, train_iou: 6.206e-01\n",
      "Epoch 6/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 2.328e-01, train_iou: 6.248e-01\n",
      "Epoch 6/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 2.285e-01, train_iou: 6.307e-01\n",
      "Epoch 6/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 2.248e-01, train_iou: 6.359e-01\n",
      "Epoch 6/100 | Batch 20/63 | Took 0.67s | train_dice_loss: 2.254e-01, train_iou: 6.349e-01\n",
      "Epoch 6/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 2.234e-01, train_iou: 6.375e-01\n",
      "Epoch 6/100 | Batch 22/63 | Took 0.62s | train_dice_loss: 2.211e-01, train_iou: 6.406e-01\n",
      "Epoch 6/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 2.222e-01, train_iou: 6.391e-01\n",
      "Epoch 6/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 2.265e-01, train_iou: 6.337e-01\n",
      "Epoch 6/100 | Batch 25/63 | Took 0.64s | train_dice_loss: 2.255e-01, train_iou: 6.349e-01\n",
      "Epoch 6/100 | Batch 26/63 | Took 0.62s | train_dice_loss: 2.267e-01, train_iou: 6.332e-01\n",
      "Epoch 6/100 | Batch 27/63 | Took 0.62s | train_dice_loss: 2.259e-01, train_iou: 6.342e-01\n",
      "Epoch 6/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 2.259e-01, train_iou: 6.342e-01\n",
      "Epoch 6/100 | Batch 29/63 | Took 0.62s | train_dice_loss: 2.249e-01, train_iou: 6.354e-01\n",
      "Epoch 6/100 | Batch 30/63 | Took 0.62s | train_dice_loss: 2.250e-01, train_iou: 6.352e-01\n",
      "Epoch 6/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 2.242e-01, train_iou: 6.363e-01\n",
      "Epoch 6/100 | Batch 32/63 | Took 0.67s | train_dice_loss: 2.232e-01, train_iou: 6.375e-01\n",
      "Epoch 6/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 2.226e-01, train_iou: 6.382e-01\n",
      "Epoch 6/100 | Batch 34/63 | Took 0.62s | train_dice_loss: 2.217e-01, train_iou: 6.395e-01\n",
      "Epoch 6/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 2.208e-01, train_iou: 6.406e-01\n",
      "Epoch 6/100 | Batch 36/63 | Took 0.62s | train_dice_loss: 2.203e-01, train_iou: 6.412e-01\n",
      "Epoch 6/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 2.197e-01, train_iou: 6.420e-01\n",
      "Epoch 6/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 2.186e-01, train_iou: 6.435e-01\n",
      "Epoch 6/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 2.196e-01, train_iou: 6.421e-01\n",
      "Epoch 6/100 | Batch 40/63 | Took 0.62s | train_dice_loss: 2.209e-01, train_iou: 6.404e-01\n",
      "Epoch 6/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 2.209e-01, train_iou: 6.403e-01\n",
      "Epoch 6/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 2.208e-01, train_iou: 6.404e-01\n",
      "Epoch 6/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 2.201e-01, train_iou: 6.413e-01\n",
      "Epoch 6/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 2.194e-01, train_iou: 6.422e-01\n",
      "Epoch 6/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 2.192e-01, train_iou: 6.425e-01\n",
      "Epoch 6/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 2.194e-01, train_iou: 6.422e-01\n",
      "Epoch 6/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 2.194e-01, train_iou: 6.421e-01\n",
      "Epoch 6/100 | Batch 48/63 | Took 0.68s | train_dice_loss: 2.188e-01, train_iou: 6.429e-01\n",
      "Epoch 6/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 2.181e-01, train_iou: 6.439e-01\n",
      "Epoch 6/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 2.173e-01, train_iou: 6.449e-01\n",
      "Epoch 6/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 2.179e-01, train_iou: 6.440e-01\n",
      "Epoch 6/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 2.194e-01, train_iou: 6.422e-01\n",
      "Epoch 6/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 2.178e-01, train_iou: 6.445e-01\n",
      "Epoch 6/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 2.184e-01, train_iou: 6.436e-01\n",
      "Epoch 6/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 2.182e-01, train_iou: 6.438e-01\n",
      "Epoch 6/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 2.179e-01, train_iou: 6.442e-01\n",
      "Epoch 6/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 2.180e-01, train_iou: 6.440e-01\n",
      "Epoch 6/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 2.182e-01, train_iou: 6.438e-01\n",
      "Epoch 6/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 2.175e-01, train_iou: 6.447e-01\n",
      "Epoch 6/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 2.167e-01, train_iou: 6.459e-01\n",
      "Epoch 6/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 2.166e-01, train_iou: 6.459e-01\n",
      "Epoch 6/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 2.162e-01, train_iou: 6.465e-01\n",
      "Epoch 6/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 2.164e-01, train_iou: 6.462e-01\n",
      "Epoch 6/100 | Took 138.35s | val_dice_loss: 2.082e-01, val_iou: 6.649e-01\n",
      "====================\n",
      "Epoch 7/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 2.167e-01, train_iou: 6.443e-01\n",
      "Epoch 7/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 2.129e-01, train_iou: 6.492e-01\n",
      "Epoch 7/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 2.219e-01, train_iou: 6.373e-01\n",
      "Epoch 7/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 2.100e-01, train_iou: 6.539e-01\n",
      "Epoch 7/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 2.082e-01, train_iou: 6.562e-01\n",
      "Epoch 7/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 2.073e-01, train_iou: 6.574e-01\n",
      "Epoch 7/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 2.036e-01, train_iou: 6.625e-01\n",
      "Epoch 7/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 1.972e-01, train_iou: 6.716e-01\n",
      "Epoch 7/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 2.013e-01, train_iou: 6.661e-01\n",
      "Epoch 7/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.987e-01, train_iou: 6.695e-01\n",
      "Epoch 7/100 | Batch 11/63 | Took 0.69s | train_dice_loss: 1.929e-01, train_iou: 6.780e-01\n",
      "Epoch 7/100 | Batch 12/63 | Took 0.63s | train_dice_loss: 1.944e-01, train_iou: 6.759e-01\n",
      "Epoch 7/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.936e-01, train_iou: 6.769e-01\n",
      "Epoch 7/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.927e-01, train_iou: 6.781e-01\n",
      "Epoch 7/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.989e-01, train_iou: 6.700e-01\n",
      "Epoch 7/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.982e-01, train_iou: 6.708e-01\n",
      "Epoch 7/100 | Batch 17/63 | Took 0.62s | train_dice_loss: 2.002e-01, train_iou: 6.680e-01\n",
      "Epoch 7/100 | Batch 18/63 | Took 0.62s | train_dice_loss: 2.007e-01, train_iou: 6.673e-01\n",
      "Epoch 7/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 2.015e-01, train_iou: 6.661e-01\n",
      "Epoch 7/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 2.052e-01, train_iou: 6.612e-01\n",
      "Epoch 7/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 2.053e-01, train_iou: 6.609e-01\n",
      "Epoch 7/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 2.047e-01, train_iou: 6.618e-01\n",
      "Epoch 7/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 2.059e-01, train_iou: 6.600e-01\n",
      "Epoch 7/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 2.081e-01, train_iou: 6.571e-01\n",
      "Epoch 7/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 2.076e-01, train_iou: 6.578e-01\n",
      "Epoch 7/100 | Batch 26/63 | Took 0.65s | train_dice_loss: 2.081e-01, train_iou: 6.570e-01\n",
      "Epoch 7/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 2.075e-01, train_iou: 6.578e-01\n",
      "Epoch 7/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 2.091e-01, train_iou: 6.556e-01\n",
      "Epoch 7/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 2.073e-01, train_iou: 6.581e-01\n",
      "Epoch 7/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 2.100e-01, train_iou: 6.546e-01\n",
      "Epoch 7/100 | Batch 31/63 | Took 0.62s | train_dice_loss: 2.122e-01, train_iou: 6.518e-01\n",
      "Epoch 7/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 2.114e-01, train_iou: 6.527e-01\n",
      "Epoch 7/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 2.100e-01, train_iou: 6.548e-01\n",
      "Epoch 7/100 | Batch 34/63 | Took 0.62s | train_dice_loss: 2.091e-01, train_iou: 6.559e-01\n",
      "Epoch 7/100 | Batch 35/63 | Took 0.62s | train_dice_loss: 2.091e-01, train_iou: 6.558e-01\n",
      "Epoch 7/100 | Batch 36/63 | Took 0.65s | train_dice_loss: 2.077e-01, train_iou: 6.579e-01\n",
      "Epoch 7/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 2.097e-01, train_iou: 6.552e-01\n",
      "Epoch 7/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 2.084e-01, train_iou: 6.570e-01\n",
      "Epoch 7/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 2.080e-01, train_iou: 6.575e-01\n",
      "Epoch 7/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 2.088e-01, train_iou: 6.565e-01\n",
      "Epoch 7/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 2.093e-01, train_iou: 6.558e-01\n",
      "Epoch 7/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 2.090e-01, train_iou: 6.561e-01\n",
      "Epoch 7/100 | Batch 43/63 | Took 0.67s | train_dice_loss: 2.087e-01, train_iou: 6.565e-01\n",
      "Epoch 7/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 2.080e-01, train_iou: 6.575e-01\n",
      "Epoch 7/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 2.066e-01, train_iou: 6.594e-01\n",
      "Epoch 7/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 2.058e-01, train_iou: 6.604e-01\n",
      "Epoch 7/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 2.070e-01, train_iou: 6.588e-01\n",
      "Epoch 7/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 2.051e-01, train_iou: 6.616e-01\n",
      "Epoch 7/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 2.057e-01, train_iou: 6.608e-01\n",
      "Epoch 7/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 2.081e-01, train_iou: 6.577e-01\n",
      "Epoch 7/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 2.077e-01, train_iou: 6.582e-01\n",
      "Epoch 7/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 2.074e-01, train_iou: 6.587e-01\n",
      "Epoch 7/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 2.076e-01, train_iou: 6.584e-01\n",
      "Epoch 7/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 2.076e-01, train_iou: 6.583e-01\n",
      "Epoch 7/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 2.071e-01, train_iou: 6.590e-01\n",
      "Epoch 7/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 2.079e-01, train_iou: 6.579e-01\n",
      "Epoch 7/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 2.077e-01, train_iou: 6.582e-01\n",
      "Epoch 7/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 2.078e-01, train_iou: 6.580e-01\n",
      "Epoch 7/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 2.065e-01, train_iou: 6.598e-01\n",
      "Epoch 7/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 2.057e-01, train_iou: 6.610e-01\n",
      "Epoch 7/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 2.063e-01, train_iou: 6.601e-01\n",
      "Epoch 7/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 2.054e-01, train_iou: 6.614e-01\n",
      "Epoch 7/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 2.047e-01, train_iou: 6.623e-01\n",
      "Epoch 7/100 | Took 138.94s | val_dice_loss: 2.008e-01, val_iou: 6.748e-01\n",
      "====================\n",
      "Epoch 8/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 2.032e-01, train_iou: 6.625e-01\n",
      "Epoch 8/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.883e-01, train_iou: 6.833e-01\n",
      "Epoch 8/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.985e-01, train_iou: 6.692e-01\n",
      "Epoch 8/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 1.841e-01, train_iou: 6.902e-01\n",
      "Epoch 8/100 | Batch 5/63 | Took 0.63s | train_dice_loss: 1.757e-01, train_iou: 7.024e-01\n",
      "Epoch 8/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.918e-01, train_iou: 6.807e-01\n",
      "Epoch 8/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 1.961e-01, train_iou: 6.744e-01\n",
      "Epoch 8/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 1.977e-01, train_iou: 6.720e-01\n",
      "Epoch 8/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 1.948e-01, train_iou: 6.759e-01\n",
      "Epoch 8/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 1.936e-01, train_iou: 6.774e-01\n",
      "Epoch 8/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 1.914e-01, train_iou: 6.803e-01\n",
      "Epoch 8/100 | Batch 12/63 | Took 0.63s | train_dice_loss: 1.919e-01, train_iou: 6.795e-01\n",
      "Epoch 8/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 1.913e-01, train_iou: 6.802e-01\n",
      "Epoch 8/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.898e-01, train_iou: 6.823e-01\n",
      "Epoch 8/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.905e-01, train_iou: 6.812e-01\n",
      "Epoch 8/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.900e-01, train_iou: 6.819e-01\n",
      "Epoch 8/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.902e-01, train_iou: 6.816e-01\n",
      "Epoch 8/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.892e-01, train_iou: 6.830e-01\n",
      "Epoch 8/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.930e-01, train_iou: 6.779e-01\n",
      "Epoch 8/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.926e-01, train_iou: 6.783e-01\n",
      "Epoch 8/100 | Batch 21/63 | Took 0.62s | train_dice_loss: 1.941e-01, train_iou: 6.762e-01\n",
      "Epoch 8/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.936e-01, train_iou: 6.768e-01\n",
      "Epoch 8/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.933e-01, train_iou: 6.772e-01\n",
      "Epoch 8/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 1.918e-01, train_iou: 6.793e-01\n",
      "Epoch 8/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.902e-01, train_iou: 6.817e-01\n",
      "Epoch 8/100 | Batch 26/63 | Took 0.62s | train_dice_loss: 1.918e-01, train_iou: 6.794e-01\n",
      "Epoch 8/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.920e-01, train_iou: 6.790e-01\n",
      "Epoch 8/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 1.918e-01, train_iou: 6.793e-01\n",
      "Epoch 8/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.940e-01, train_iou: 6.764e-01\n",
      "Epoch 8/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.940e-01, train_iou: 6.762e-01\n",
      "Epoch 8/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.943e-01, train_iou: 6.758e-01\n",
      "Epoch 8/100 | Batch 32/63 | Took 0.62s | train_dice_loss: 1.934e-01, train_iou: 6.770e-01\n",
      "Epoch 8/100 | Batch 33/63 | Took 0.67s | train_dice_loss: 1.923e-01, train_iou: 6.786e-01\n",
      "Epoch 8/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.914e-01, train_iou: 6.799e-01\n",
      "Epoch 8/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.910e-01, train_iou: 6.805e-01\n",
      "Epoch 8/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.927e-01, train_iou: 6.781e-01\n",
      "Epoch 8/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.922e-01, train_iou: 6.789e-01\n",
      "Epoch 8/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.931e-01, train_iou: 6.776e-01\n",
      "Epoch 8/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.935e-01, train_iou: 6.770e-01\n",
      "Epoch 8/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.959e-01, train_iou: 6.738e-01\n",
      "Epoch 8/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.970e-01, train_iou: 6.723e-01\n",
      "Epoch 8/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.969e-01, train_iou: 6.724e-01\n",
      "Epoch 8/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.974e-01, train_iou: 6.718e-01\n",
      "Epoch 8/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.970e-01, train_iou: 6.723e-01\n",
      "Epoch 8/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.969e-01, train_iou: 6.724e-01\n",
      "Epoch 8/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.974e-01, train_iou: 6.716e-01\n",
      "Epoch 8/100 | Batch 47/63 | Took 0.62s | train_dice_loss: 1.967e-01, train_iou: 6.727e-01\n",
      "Epoch 8/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.957e-01, train_iou: 6.741e-01\n",
      "Epoch 8/100 | Batch 49/63 | Took 0.64s | train_dice_loss: 1.978e-01, train_iou: 6.713e-01\n",
      "Epoch 8/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.984e-01, train_iou: 6.704e-01\n",
      "Epoch 8/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.996e-01, train_iou: 6.689e-01\n",
      "Epoch 8/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 2.009e-01, train_iou: 6.671e-01\n",
      "Epoch 8/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 2.004e-01, train_iou: 6.678e-01\n",
      "Epoch 8/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 2.014e-01, train_iou: 6.665e-01\n",
      "Epoch 8/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 2.008e-01, train_iou: 6.673e-01\n",
      "Epoch 8/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 2.020e-01, train_iou: 6.657e-01\n",
      "Epoch 8/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 2.022e-01, train_iou: 6.654e-01\n",
      "Epoch 8/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 2.024e-01, train_iou: 6.651e-01\n",
      "Epoch 8/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 2.024e-01, train_iou: 6.651e-01\n",
      "Epoch 8/100 | Batch 60/63 | Took 0.62s | train_dice_loss: 2.046e-01, train_iou: 6.623e-01\n",
      "Epoch 8/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 2.040e-01, train_iou: 6.631e-01\n",
      "Epoch 8/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 2.037e-01, train_iou: 6.635e-01\n",
      "Epoch 8/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 2.035e-01, train_iou: 6.637e-01\n",
      "Epoch 8/100 | Took 139.92s | val_dice_loss: 2.326e-01, val_iou: 6.317e-01\n",
      "====================\n",
      "Epoch 9/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.869e-01, train_iou: 6.852e-01\n",
      "Epoch 9/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.923e-01, train_iou: 6.776e-01\n",
      "Epoch 9/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.831e-01, train_iou: 6.908e-01\n",
      "Epoch 9/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 1.867e-01, train_iou: 6.856e-01\n",
      "Epoch 9/100 | Batch 5/63 | Took 0.63s | train_dice_loss: 1.956e-01, train_iou: 6.734e-01\n",
      "Epoch 9/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 2.125e-01, train_iou: 6.516e-01\n",
      "Epoch 9/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 2.092e-01, train_iou: 6.557e-01\n",
      "Epoch 9/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 2.087e-01, train_iou: 6.563e-01\n",
      "Epoch 9/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 2.108e-01, train_iou: 6.532e-01\n",
      "Epoch 9/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 2.074e-01, train_iou: 6.578e-01\n",
      "Epoch 9/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 2.075e-01, train_iou: 6.576e-01\n",
      "Epoch 9/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 2.050e-01, train_iou: 6.610e-01\n",
      "Epoch 9/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 2.100e-01, train_iou: 6.544e-01\n",
      "Epoch 9/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 2.094e-01, train_iou: 6.552e-01\n",
      "Epoch 9/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 2.061e-01, train_iou: 6.597e-01\n",
      "Epoch 9/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 2.057e-01, train_iou: 6.603e-01\n",
      "Epoch 9/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 2.035e-01, train_iou: 6.633e-01\n",
      "Epoch 9/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 2.019e-01, train_iou: 6.654e-01\n",
      "Epoch 9/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 2.032e-01, train_iou: 6.637e-01\n",
      "Epoch 9/100 | Batch 20/63 | Took 0.62s | train_dice_loss: 2.014e-01, train_iou: 6.661e-01\n",
      "Epoch 9/100 | Batch 21/63 | Took 0.62s | train_dice_loss: 1.998e-01, train_iou: 6.684e-01\n",
      "Epoch 9/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 2.025e-01, train_iou: 6.647e-01\n",
      "Epoch 9/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 2.018e-01, train_iou: 6.657e-01\n",
      "Epoch 9/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 2.043e-01, train_iou: 6.623e-01\n",
      "Epoch 9/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 2.048e-01, train_iou: 6.616e-01\n",
      "Epoch 9/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 2.048e-01, train_iou: 6.615e-01\n",
      "Epoch 9/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 2.068e-01, train_iou: 6.589e-01\n",
      "Epoch 9/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 2.056e-01, train_iou: 6.605e-01\n",
      "Epoch 9/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 2.069e-01, train_iou: 6.586e-01\n",
      "Epoch 9/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 2.098e-01, train_iou: 6.550e-01\n",
      "Epoch 9/100 | Batch 31/63 | Took 0.62s | train_dice_loss: 2.084e-01, train_iou: 6.569e-01\n",
      "Epoch 9/100 | Batch 32/63 | Took 0.62s | train_dice_loss: 2.075e-01, train_iou: 6.581e-01\n",
      "Epoch 9/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 2.066e-01, train_iou: 6.593e-01\n",
      "Epoch 9/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 2.055e-01, train_iou: 6.607e-01\n",
      "Epoch 9/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 2.042e-01, train_iou: 6.626e-01\n",
      "Epoch 9/100 | Batch 36/63 | Took 0.62s | train_dice_loss: 2.036e-01, train_iou: 6.633e-01\n",
      "Epoch 9/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 2.025e-01, train_iou: 6.649e-01\n",
      "Epoch 9/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 2.018e-01, train_iou: 6.659e-01\n",
      "Epoch 9/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 2.019e-01, train_iou: 6.656e-01\n",
      "Epoch 9/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 2.014e-01, train_iou: 6.663e-01\n",
      "Epoch 9/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 2.011e-01, train_iou: 6.667e-01\n",
      "Epoch 9/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 2.005e-01, train_iou: 6.675e-01\n",
      "Epoch 9/100 | Batch 43/63 | Took 0.62s | train_dice_loss: 2.011e-01, train_iou: 6.667e-01\n",
      "Epoch 9/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 2.005e-01, train_iou: 6.675e-01\n",
      "Epoch 9/100 | Batch 45/63 | Took 0.62s | train_dice_loss: 2.001e-01, train_iou: 6.681e-01\n",
      "Epoch 9/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.999e-01, train_iou: 6.682e-01\n",
      "Epoch 9/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 2.015e-01, train_iou: 6.662e-01\n",
      "Epoch 9/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 2.018e-01, train_iou: 6.657e-01\n",
      "Epoch 9/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 2.011e-01, train_iou: 6.666e-01\n",
      "Epoch 9/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 2.007e-01, train_iou: 6.672e-01\n",
      "Epoch 9/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.999e-01, train_iou: 6.683e-01\n",
      "Epoch 9/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.984e-01, train_iou: 6.706e-01\n",
      "Epoch 9/100 | Batch 53/63 | Took 0.62s | train_dice_loss: 1.994e-01, train_iou: 6.692e-01\n",
      "Epoch 9/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.986e-01, train_iou: 6.703e-01\n",
      "Epoch 9/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.984e-01, train_iou: 6.705e-01\n",
      "Epoch 9/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.977e-01, train_iou: 6.715e-01\n",
      "Epoch 9/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.980e-01, train_iou: 6.711e-01\n",
      "Epoch 9/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.981e-01, train_iou: 6.709e-01\n",
      "Epoch 9/100 | Batch 59/63 | Took 0.68s | train_dice_loss: 1.991e-01, train_iou: 6.696e-01\n",
      "Epoch 9/100 | Batch 60/63 | Took 0.68s | train_dice_loss: 1.990e-01, train_iou: 6.697e-01\n",
      "Epoch 9/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.986e-01, train_iou: 6.702e-01\n",
      "Epoch 9/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.982e-01, train_iou: 6.707e-01\n",
      "Epoch 9/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.979e-01, train_iou: 6.712e-01\n",
      "Epoch 9/100 | Took 139.34s | val_dice_loss: 2.054e-01, val_iou: 6.689e-01\n",
      "====================\n",
      "Epoch 10/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 2.060e-01, train_iou: 6.584e-01\n",
      "Epoch 10/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.979e-01, train_iou: 6.698e-01\n",
      "Epoch 10/100 | Batch 3/63 | Took 0.69s | train_dice_loss: 1.992e-01, train_iou: 6.680e-01\n",
      "Epoch 10/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 2.247e-01, train_iou: 6.352e-01\n",
      "Epoch 10/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 2.102e-01, train_iou: 6.555e-01\n",
      "Epoch 10/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 2.003e-01, train_iou: 6.692e-01\n",
      "Epoch 10/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.994e-01, train_iou: 6.700e-01\n",
      "Epoch 10/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 2.019e-01, train_iou: 6.663e-01\n",
      "Epoch 10/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 2.045e-01, train_iou: 6.626e-01\n",
      "Epoch 10/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 2.005e-01, train_iou: 6.681e-01\n",
      "Epoch 10/100 | Batch 11/63 | Took 0.63s | train_dice_loss: 2.022e-01, train_iou: 6.655e-01\n",
      "Epoch 10/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 1.958e-01, train_iou: 6.748e-01\n",
      "Epoch 10/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.950e-01, train_iou: 6.757e-01\n",
      "Epoch 10/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.944e-01, train_iou: 6.764e-01\n",
      "Epoch 10/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.929e-01, train_iou: 6.785e-01\n",
      "Epoch 10/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.924e-01, train_iou: 6.791e-01\n",
      "Epoch 10/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.918e-01, train_iou: 6.798e-01\n",
      "Epoch 10/100 | Batch 18/63 | Took 0.68s | train_dice_loss: 1.915e-01, train_iou: 6.802e-01\n",
      "Epoch 10/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.899e-01, train_iou: 6.825e-01\n",
      "Epoch 10/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.910e-01, train_iou: 6.808e-01\n",
      "Epoch 10/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.898e-01, train_iou: 6.825e-01\n",
      "Epoch 10/100 | Batch 22/63 | Took 0.62s | train_dice_loss: 1.891e-01, train_iou: 6.834e-01\n",
      "Epoch 10/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.898e-01, train_iou: 6.823e-01\n",
      "Epoch 10/100 | Batch 24/63 | Took 0.62s | train_dice_loss: 1.889e-01, train_iou: 6.836e-01\n",
      "Epoch 10/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.875e-01, train_iou: 6.856e-01\n",
      "Epoch 10/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.874e-01, train_iou: 6.857e-01\n",
      "Epoch 10/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.874e-01, train_iou: 6.857e-01\n",
      "Epoch 10/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 1.873e-01, train_iou: 6.857e-01\n",
      "Epoch 10/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.875e-01, train_iou: 6.854e-01\n",
      "Epoch 10/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.872e-01, train_iou: 6.859e-01\n",
      "Epoch 10/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.894e-01, train_iou: 6.828e-01\n",
      "Epoch 10/100 | Batch 32/63 | Took 0.62s | train_dice_loss: 1.885e-01, train_iou: 6.842e-01\n",
      "Epoch 10/100 | Batch 33/63 | Took 0.62s | train_dice_loss: 1.888e-01, train_iou: 6.837e-01\n",
      "Epoch 10/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.876e-01, train_iou: 6.853e-01\n",
      "Epoch 10/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.885e-01, train_iou: 6.841e-01\n",
      "Epoch 10/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.892e-01, train_iou: 6.831e-01\n",
      "Epoch 10/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.886e-01, train_iou: 6.839e-01\n",
      "Epoch 10/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.878e-01, train_iou: 6.850e-01\n",
      "Epoch 10/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.884e-01, train_iou: 6.842e-01\n",
      "Epoch 10/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.886e-01, train_iou: 6.839e-01\n",
      "Epoch 10/100 | Batch 41/63 | Took 0.66s | train_dice_loss: 1.882e-01, train_iou: 6.844e-01\n",
      "Epoch 10/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.898e-01, train_iou: 6.823e-01\n",
      "Epoch 10/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.900e-01, train_iou: 6.819e-01\n",
      "Epoch 10/100 | Batch 44/63 | Took 0.62s | train_dice_loss: 1.895e-01, train_iou: 6.826e-01\n",
      "Epoch 10/100 | Batch 45/63 | Took 0.62s | train_dice_loss: 1.898e-01, train_iou: 6.822e-01\n",
      "Epoch 10/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.899e-01, train_iou: 6.821e-01\n",
      "Epoch 10/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.901e-01, train_iou: 6.818e-01\n",
      "Epoch 10/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.898e-01, train_iou: 6.822e-01\n",
      "Epoch 10/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.889e-01, train_iou: 6.835e-01\n",
      "Epoch 10/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.878e-01, train_iou: 6.850e-01\n",
      "Epoch 10/100 | Batch 51/63 | Took 0.62s | train_dice_loss: 1.891e-01, train_iou: 6.833e-01\n",
      "Epoch 10/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.885e-01, train_iou: 6.841e-01\n",
      "Epoch 10/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.889e-01, train_iou: 6.836e-01\n",
      "Epoch 10/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.883e-01, train_iou: 6.844e-01\n",
      "Epoch 10/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.875e-01, train_iou: 6.855e-01\n",
      "Epoch 10/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.873e-01, train_iou: 6.859e-01\n",
      "Epoch 10/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.884e-01, train_iou: 6.843e-01\n",
      "Epoch 10/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.887e-01, train_iou: 6.838e-01\n",
      "Epoch 10/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.879e-01, train_iou: 6.850e-01\n",
      "Epoch 10/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.886e-01, train_iou: 6.841e-01\n",
      "Epoch 10/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.889e-01, train_iou: 6.836e-01\n",
      "Epoch 10/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.887e-01, train_iou: 6.839e-01\n",
      "Epoch 10/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.902e-01, train_iou: 6.819e-01\n",
      "Epoch 10/100 | Took 138.46s | val_dice_loss: 1.883e-01, val_iou: 6.932e-01\n",
      "====================\n",
      "Epoch 11/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.259e-01, train_iou: 7.764e-01\n",
      "Epoch 11/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.784e-01, train_iou: 7.007e-01\n",
      "Epoch 11/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.908e-01, train_iou: 6.822e-01\n",
      "Epoch 11/100 | Batch 4/63 | Took 0.63s | train_dice_loss: 1.851e-01, train_iou: 6.896e-01\n",
      "Epoch 11/100 | Batch 5/63 | Took 0.63s | train_dice_loss: 1.775e-01, train_iou: 7.005e-01\n",
      "Epoch 11/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.757e-01, train_iou: 7.028e-01\n",
      "Epoch 11/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.023e-01\n",
      "Epoch 11/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 1.864e-01, train_iou: 6.880e-01\n",
      "Epoch 11/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 1.851e-01, train_iou: 6.896e-01\n",
      "Epoch 11/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 1.840e-01, train_iou: 6.910e-01\n",
      "Epoch 11/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 1.949e-01, train_iou: 6.767e-01\n",
      "Epoch 11/100 | Batch 12/63 | Took 0.63s | train_dice_loss: 1.911e-01, train_iou: 6.819e-01\n",
      "Epoch 11/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 1.961e-01, train_iou: 6.750e-01\n",
      "Epoch 11/100 | Batch 14/63 | Took 0.62s | train_dice_loss: 1.931e-01, train_iou: 6.793e-01\n",
      "Epoch 11/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.892e-01, train_iou: 6.848e-01\n",
      "Epoch 11/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.868e-01, train_iou: 6.881e-01\n",
      "Epoch 11/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.871e-01, train_iou: 6.876e-01\n",
      "Epoch 11/100 | Batch 18/63 | Took 0.62s | train_dice_loss: 1.873e-01, train_iou: 6.872e-01\n",
      "Epoch 11/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.886e-01, train_iou: 6.852e-01\n",
      "Epoch 11/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.925e-01, train_iou: 6.799e-01\n",
      "Epoch 11/100 | Batch 21/63 | Took 0.66s | train_dice_loss: 1.918e-01, train_iou: 6.808e-01\n",
      "Epoch 11/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.887e-01, train_iou: 6.853e-01\n",
      "Epoch 11/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.890e-01, train_iou: 6.847e-01\n",
      "Epoch 11/100 | Batch 24/63 | Took 0.62s | train_dice_loss: 1.876e-01, train_iou: 6.866e-01\n",
      "Epoch 11/100 | Batch 25/63 | Took 0.62s | train_dice_loss: 1.887e-01, train_iou: 6.850e-01\n",
      "Epoch 11/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.882e-01, train_iou: 6.856e-01\n",
      "Epoch 11/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.883e-01, train_iou: 6.855e-01\n",
      "Epoch 11/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 1.878e-01, train_iou: 6.860e-01\n",
      "Epoch 11/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.861e-01, train_iou: 6.884e-01\n",
      "Epoch 11/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.871e-01, train_iou: 6.870e-01\n",
      "Epoch 11/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.873e-01, train_iou: 6.867e-01\n",
      "Epoch 11/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.856e-01, train_iou: 6.891e-01\n",
      "Epoch 11/100 | Batch 33/63 | Took 0.62s | train_dice_loss: 1.847e-01, train_iou: 6.904e-01\n",
      "Epoch 11/100 | Batch 34/63 | Took 0.62s | train_dice_loss: 1.833e-01, train_iou: 6.924e-01\n",
      "Epoch 11/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.841e-01, train_iou: 6.912e-01\n",
      "Epoch 11/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.827e-01, train_iou: 6.933e-01\n",
      "Epoch 11/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.837e-01, train_iou: 6.918e-01\n",
      "Epoch 11/100 | Batch 38/63 | Took 0.62s | train_dice_loss: 1.839e-01, train_iou: 6.915e-01\n",
      "Epoch 11/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.825e-01, train_iou: 6.935e-01\n",
      "Epoch 11/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.827e-01, train_iou: 6.932e-01\n",
      "Epoch 11/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.823e-01, train_iou: 6.937e-01\n",
      "Epoch 11/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.823e-01, train_iou: 6.936e-01\n",
      "Epoch 11/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.828e-01, train_iou: 6.930e-01\n",
      "Epoch 11/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.822e-01, train_iou: 6.938e-01\n",
      "Epoch 11/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.820e-01, train_iou: 6.940e-01\n",
      "Epoch 11/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.817e-01, train_iou: 6.943e-01\n",
      "Epoch 11/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.818e-01, train_iou: 6.942e-01\n",
      "Epoch 11/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.811e-01, train_iou: 6.952e-01\n",
      "Epoch 11/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.811e-01, train_iou: 6.952e-01\n",
      "Epoch 11/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.803e-01, train_iou: 6.964e-01\n",
      "Epoch 11/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.802e-01, train_iou: 6.964e-01\n",
      "Epoch 11/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.798e-01, train_iou: 6.969e-01\n",
      "Epoch 11/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.795e-01, train_iou: 6.975e-01\n",
      "Epoch 11/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.813e-01, train_iou: 6.950e-01\n",
      "Epoch 11/100 | Batch 55/63 | Took 0.62s | train_dice_loss: 1.820e-01, train_iou: 6.940e-01\n",
      "Epoch 11/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.826e-01, train_iou: 6.931e-01\n",
      "Epoch 11/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.825e-01, train_iou: 6.932e-01\n",
      "Epoch 11/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.819e-01, train_iou: 6.940e-01\n",
      "Epoch 11/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.819e-01, train_iou: 6.940e-01\n",
      "Epoch 11/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.825e-01, train_iou: 6.932e-01\n",
      "Epoch 11/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.824e-01, train_iou: 6.933e-01\n",
      "Epoch 11/100 | Batch 62/63 | Took 0.62s | train_dice_loss: 1.827e-01, train_iou: 6.928e-01\n",
      "Epoch 11/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.825e-01, train_iou: 6.931e-01\n",
      "Epoch 11/100 | Took 139.63s | val_dice_loss: 2.086e-01, val_iou: 6.677e-01\n",
      "====================\n",
      "Epoch 12/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.956e-01, train_iou: 6.727e-01\n",
      "Epoch 12/100 | Batch 2/63 | Took 0.64s | train_dice_loss: 1.975e-01, train_iou: 6.703e-01\n",
      "Epoch 12/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.946e-01, train_iou: 6.743e-01\n",
      "Epoch 12/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 2.064e-01, train_iou: 6.584e-01\n",
      "Epoch 12/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.991e-01, train_iou: 6.686e-01\n",
      "Epoch 12/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.941e-01, train_iou: 6.756e-01\n",
      "Epoch 12/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.831e-01, train_iou: 6.921e-01\n",
      "Epoch 12/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 1.875e-01, train_iou: 6.859e-01\n",
      "Epoch 12/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 1.867e-01, train_iou: 6.867e-01\n",
      "Epoch 12/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.877e-01, train_iou: 6.852e-01\n",
      "Epoch 12/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 1.960e-01, train_iou: 6.742e-01\n",
      "Epoch 12/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 1.922e-01, train_iou: 6.795e-01\n",
      "Epoch 12/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 1.911e-01, train_iou: 6.809e-01\n",
      "Epoch 12/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.880e-01, train_iou: 6.853e-01\n",
      "Epoch 12/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.859e-01, train_iou: 6.884e-01\n",
      "Epoch 12/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.881e-01, train_iou: 6.852e-01\n",
      "Epoch 12/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.873e-01, train_iou: 6.862e-01\n",
      "Epoch 12/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.925e-01, train_iou: 6.792e-01\n",
      "Epoch 12/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.931e-01, train_iou: 6.784e-01\n",
      "Epoch 12/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.905e-01, train_iou: 6.820e-01\n",
      "Epoch 12/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.916e-01, train_iou: 6.804e-01\n",
      "Epoch 12/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.928e-01, train_iou: 6.787e-01\n",
      "Epoch 12/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.951e-01, train_iou: 6.754e-01\n",
      "Epoch 12/100 | Batch 24/63 | Took 0.62s | train_dice_loss: 1.949e-01, train_iou: 6.757e-01\n",
      "Epoch 12/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.950e-01, train_iou: 6.755e-01\n",
      "Epoch 12/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.948e-01, train_iou: 6.757e-01\n",
      "Epoch 12/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.936e-01, train_iou: 6.773e-01\n",
      "Epoch 12/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 1.933e-01, train_iou: 6.777e-01\n",
      "Epoch 12/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.924e-01, train_iou: 6.789e-01\n",
      "Epoch 12/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.921e-01, train_iou: 6.794e-01\n",
      "Epoch 12/100 | Batch 31/63 | Took 0.67s | train_dice_loss: 1.924e-01, train_iou: 6.789e-01\n",
      "Epoch 12/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.910e-01, train_iou: 6.808e-01\n",
      "Epoch 12/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.901e-01, train_iou: 6.821e-01\n",
      "Epoch 12/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.899e-01, train_iou: 6.824e-01\n",
      "Epoch 12/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.891e-01, train_iou: 6.835e-01\n",
      "Epoch 12/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.881e-01, train_iou: 6.849e-01\n",
      "Epoch 12/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.884e-01, train_iou: 6.844e-01\n",
      "Epoch 12/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.886e-01, train_iou: 6.841e-01\n",
      "Epoch 12/100 | Batch 39/63 | Took 0.62s | train_dice_loss: 1.875e-01, train_iou: 6.857e-01\n",
      "Epoch 12/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.869e-01, train_iou: 6.865e-01\n",
      "Epoch 12/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.862e-01, train_iou: 6.875e-01\n",
      "Epoch 12/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.873e-01, train_iou: 6.859e-01\n",
      "Epoch 12/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.865e-01, train_iou: 6.872e-01\n",
      "Epoch 12/100 | Batch 44/63 | Took 0.65s | train_dice_loss: 1.862e-01, train_iou: 6.875e-01\n",
      "Epoch 12/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.856e-01, train_iou: 6.884e-01\n",
      "Epoch 12/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.848e-01, train_iou: 6.894e-01\n",
      "Epoch 12/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.850e-01, train_iou: 6.892e-01\n",
      "Epoch 12/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.870e-01, train_iou: 6.865e-01\n",
      "Epoch 12/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.861e-01, train_iou: 6.879e-01\n",
      "Epoch 12/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.853e-01, train_iou: 6.890e-01\n",
      "Epoch 12/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.841e-01, train_iou: 6.907e-01\n",
      "Epoch 12/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.835e-01, train_iou: 6.917e-01\n",
      "Epoch 12/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.825e-01, train_iou: 6.931e-01\n",
      "Epoch 12/100 | Batch 54/63 | Took 0.62s | train_dice_loss: 1.827e-01, train_iou: 6.927e-01\n",
      "Epoch 12/100 | Batch 55/63 | Took 0.62s | train_dice_loss: 1.832e-01, train_iou: 6.921e-01\n",
      "Epoch 12/100 | Batch 56/63 | Took 0.62s | train_dice_loss: 1.838e-01, train_iou: 6.912e-01\n",
      "Epoch 12/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.841e-01, train_iou: 6.907e-01\n",
      "Epoch 12/100 | Batch 58/63 | Took 0.66s | train_dice_loss: 1.834e-01, train_iou: 6.918e-01\n",
      "Epoch 12/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.838e-01, train_iou: 6.911e-01\n",
      "Epoch 12/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.840e-01, train_iou: 6.909e-01\n",
      "Epoch 12/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.838e-01, train_iou: 6.911e-01\n",
      "Epoch 12/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.833e-01, train_iou: 6.918e-01\n",
      "Epoch 12/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.831e-01, train_iou: 6.920e-01\n",
      "Epoch 12/100 | Took 139.10s | val_dice_loss: 1.895e-01, val_iou: 6.910e-01\n",
      "====================\n",
      "Epoch 13/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.377e-01, train_iou: 7.582e-01\n",
      "Epoch 13/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.871e-01, train_iou: 6.881e-01\n",
      "Epoch 13/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.947e-01, train_iou: 6.764e-01\n",
      "Epoch 13/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 1.835e-01, train_iou: 6.921e-01\n",
      "Epoch 13/100 | Batch 5/63 | Took 0.63s | train_dice_loss: 1.846e-01, train_iou: 6.901e-01\n",
      "Epoch 13/100 | Batch 6/63 | Took 0.63s | train_dice_loss: 1.809e-01, train_iou: 6.952e-01\n",
      "Epoch 13/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.814e-01, train_iou: 6.942e-01\n",
      "Epoch 13/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 1.874e-01, train_iou: 6.859e-01\n",
      "Epoch 13/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 1.798e-01, train_iou: 6.972e-01\n",
      "Epoch 13/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.755e-01, train_iou: 7.034e-01\n",
      "Epoch 13/100 | Batch 11/63 | Took 0.63s | train_dice_loss: 1.751e-01, train_iou: 7.037e-01\n",
      "Epoch 13/100 | Batch 12/63 | Took 0.65s | train_dice_loss: 1.767e-01, train_iou: 7.014e-01\n",
      "Epoch 13/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.765e-01, train_iou: 7.015e-01\n",
      "Epoch 13/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.778e-01, train_iou: 6.995e-01\n",
      "Epoch 13/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 1.761e-01, train_iou: 7.020e-01\n",
      "Epoch 13/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.743e-01, train_iou: 7.046e-01\n",
      "Epoch 13/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.739e-01, train_iou: 7.051e-01\n",
      "Epoch 13/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.717e-01, train_iou: 7.084e-01\n",
      "Epoch 13/100 | Batch 19/63 | Took 0.67s | train_dice_loss: 1.739e-01, train_iou: 7.052e-01\n",
      "Epoch 13/100 | Batch 20/63 | Took 0.62s | train_dice_loss: 1.725e-01, train_iou: 7.071e-01\n",
      "Epoch 13/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.732e-01, train_iou: 7.061e-01\n",
      "Epoch 13/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.780e-01, train_iou: 6.996e-01\n",
      "Epoch 13/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.783e-01, train_iou: 6.991e-01\n",
      "Epoch 13/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 1.771e-01, train_iou: 7.008e-01\n",
      "Epoch 13/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.767e-01, train_iou: 7.013e-01\n",
      "Epoch 13/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.756e-01, train_iou: 7.029e-01\n",
      "Epoch 13/100 | Batch 27/63 | Took 0.62s | train_dice_loss: 1.755e-01, train_iou: 7.030e-01\n",
      "Epoch 13/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 1.751e-01, train_iou: 7.035e-01\n",
      "Epoch 13/100 | Batch 29/63 | Took 0.62s | train_dice_loss: 1.736e-01, train_iou: 7.057e-01\n",
      "Epoch 13/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.735e-01, train_iou: 7.058e-01\n",
      "Epoch 13/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.729e-01, train_iou: 7.067e-01\n",
      "Epoch 13/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.719e-01, train_iou: 7.081e-01\n",
      "Epoch 13/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.716e-01, train_iou: 7.085e-01\n",
      "Epoch 13/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.712e-01, train_iou: 7.091e-01\n",
      "Epoch 13/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.702e-01, train_iou: 7.105e-01\n",
      "Epoch 13/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.712e-01, train_iou: 7.091e-01\n",
      "Epoch 13/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.707e-01, train_iou: 7.098e-01\n",
      "Epoch 13/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.692e-01, train_iou: 7.121e-01\n",
      "Epoch 13/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.701e-01, train_iou: 7.107e-01\n",
      "Epoch 13/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.698e-01, train_iou: 7.112e-01\n",
      "Epoch 13/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.701e-01, train_iou: 7.107e-01\n",
      "Epoch 13/100 | Batch 42/63 | Took 0.65s | train_dice_loss: 1.702e-01, train_iou: 7.105e-01\n",
      "Epoch 13/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.705e-01, train_iou: 7.100e-01\n",
      "Epoch 13/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.713e-01, train_iou: 7.088e-01\n",
      "Epoch 13/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.717e-01, train_iou: 7.083e-01\n",
      "Epoch 13/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.711e-01, train_iou: 7.091e-01\n",
      "Epoch 13/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.727e-01, train_iou: 7.069e-01\n",
      "Epoch 13/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.741e-01, train_iou: 7.050e-01\n",
      "Epoch 13/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.750e-01, train_iou: 7.037e-01\n",
      "Epoch 13/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.751e-01, train_iou: 7.035e-01\n",
      "Epoch 13/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.024e-01\n",
      "Epoch 13/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.023e-01\n",
      "Epoch 13/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.757e-01, train_iou: 7.026e-01\n",
      "Epoch 13/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.748e-01, train_iou: 7.039e-01\n",
      "Epoch 13/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.740e-01, train_iou: 7.051e-01\n",
      "Epoch 13/100 | Batch 56/63 | Took 0.62s | train_dice_loss: 1.742e-01, train_iou: 7.048e-01\n",
      "Epoch 13/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.744e-01, train_iou: 7.045e-01\n",
      "Epoch 13/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.746e-01, train_iou: 7.042e-01\n",
      "Epoch 13/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.749e-01, train_iou: 7.037e-01\n",
      "Epoch 13/100 | Batch 60/63 | Took 0.64s | train_dice_loss: 1.741e-01, train_iou: 7.048e-01\n",
      "Epoch 13/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.741e-01, train_iou: 7.049e-01\n",
      "Epoch 13/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.743e-01, train_iou: 7.046e-01\n",
      "Epoch 13/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.731e-01, train_iou: 7.064e-01\n",
      "Epoch 13/100 | Took 138.51s | val_dice_loss: 1.864e-01, val_iou: 6.960e-01\n",
      "====================\n",
      "Epoch 14/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 2.062e-01, train_iou: 6.581e-01\n",
      "Epoch 14/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.565e-01, train_iou: 7.326e-01\n",
      "Epoch 14/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.533e-01, train_iou: 7.364e-01\n",
      "Epoch 14/100 | Batch 4/63 | Took 0.63s | train_dice_loss: 1.434e-01, train_iou: 7.513e-01\n",
      "Epoch 14/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.737e-01, train_iou: 7.099e-01\n",
      "Epoch 14/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.786e-01, train_iou: 7.020e-01\n",
      "Epoch 14/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.845e-01, train_iou: 6.931e-01\n",
      "Epoch 14/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 1.809e-01, train_iou: 6.978e-01\n",
      "Epoch 14/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 1.767e-01, train_iou: 7.036e-01\n",
      "Epoch 14/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.886e-01, train_iou: 6.876e-01\n",
      "Epoch 14/100 | Batch 11/63 | Took 0.63s | train_dice_loss: 1.863e-01, train_iou: 6.906e-01\n",
      "Epoch 14/100 | Batch 12/63 | Took 0.63s | train_dice_loss: 1.889e-01, train_iou: 6.866e-01\n",
      "Epoch 14/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.846e-01, train_iou: 6.926e-01\n",
      "Epoch 14/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.823e-01, train_iou: 6.957e-01\n",
      "Epoch 14/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.791e-01, train_iou: 7.002e-01\n",
      "Epoch 14/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.831e-01, train_iou: 6.945e-01\n",
      "Epoch 14/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.815e-01, train_iou: 6.966e-01\n",
      "Epoch 14/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.793e-01, train_iou: 6.996e-01\n",
      "Epoch 14/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.779e-01, train_iou: 7.015e-01\n",
      "Epoch 14/100 | Batch 20/63 | Took 0.62s | train_dice_loss: 1.778e-01, train_iou: 7.014e-01\n",
      "Epoch 14/100 | Batch 21/63 | Took 0.62s | train_dice_loss: 1.767e-01, train_iou: 7.029e-01\n",
      "Epoch 14/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.749e-01, train_iou: 7.055e-01\n",
      "Epoch 14/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.731e-01, train_iou: 7.080e-01\n",
      "Epoch 14/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 1.747e-01, train_iou: 7.057e-01\n",
      "Epoch 14/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.733e-01, train_iou: 7.076e-01\n",
      "Epoch 14/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.744e-01, train_iou: 7.060e-01\n",
      "Epoch 14/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.785e-01, train_iou: 7.004e-01\n",
      "Epoch 14/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 1.786e-01, train_iou: 7.002e-01\n",
      "Epoch 14/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.787e-01, train_iou: 6.999e-01\n",
      "Epoch 14/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.804e-01, train_iou: 6.975e-01\n",
      "Epoch 14/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.814e-01, train_iou: 6.960e-01\n",
      "Epoch 14/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.821e-01, train_iou: 6.949e-01\n",
      "Epoch 14/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.817e-01, train_iou: 6.954e-01\n",
      "Epoch 14/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.807e-01, train_iou: 6.968e-01\n",
      "Epoch 14/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.804e-01, train_iou: 6.971e-01\n",
      "Epoch 14/100 | Batch 36/63 | Took 0.62s | train_dice_loss: 1.791e-01, train_iou: 6.990e-01\n",
      "Epoch 14/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.800e-01, train_iou: 6.976e-01\n",
      "Epoch 14/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.797e-01, train_iou: 6.980e-01\n",
      "Epoch 14/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.787e-01, train_iou: 6.994e-01\n",
      "Epoch 14/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.776e-01, train_iou: 7.010e-01\n",
      "Epoch 14/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.779e-01, train_iou: 7.006e-01\n",
      "Epoch 14/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.764e-01, train_iou: 7.027e-01\n",
      "Epoch 14/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.761e-01, train_iou: 7.031e-01\n",
      "Epoch 14/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.757e-01, train_iou: 7.036e-01\n",
      "Epoch 14/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.771e-01, train_iou: 7.017e-01\n",
      "Epoch 14/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.762e-01, train_iou: 7.030e-01\n",
      "Epoch 14/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.754e-01, train_iou: 7.041e-01\n",
      "Epoch 14/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.750e-01, train_iou: 7.046e-01\n",
      "Epoch 14/100 | Batch 49/63 | Took 0.62s | train_dice_loss: 1.754e-01, train_iou: 7.040e-01\n",
      "Epoch 14/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.766e-01, train_iou: 7.024e-01\n",
      "Epoch 14/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.765e-01, train_iou: 7.023e-01\n",
      "Epoch 14/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.761e-01, train_iou: 7.030e-01\n",
      "Epoch 14/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.761e-01, train_iou: 7.028e-01\n",
      "Epoch 14/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.031e-01\n",
      "Epoch 14/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.755e-01, train_iou: 7.038e-01\n",
      "Epoch 14/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.756e-01, train_iou: 7.035e-01\n",
      "Epoch 14/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.763e-01, train_iou: 7.025e-01\n",
      "Epoch 14/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.775e-01, train_iou: 7.008e-01\n",
      "Epoch 14/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.779e-01, train_iou: 7.002e-01\n",
      "Epoch 14/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.779e-01, train_iou: 7.002e-01\n",
      "Epoch 14/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.773e-01, train_iou: 7.010e-01\n",
      "Epoch 14/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.763e-01, train_iou: 7.025e-01\n",
      "Epoch 14/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.760e-01, train_iou: 7.029e-01\n",
      "Epoch 14/100 | Took 138.56s | val_dice_loss: 1.837e-01, val_iou: 7.000e-01\n",
      "====================\n",
      "Epoch 15/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.799e-01, train_iou: 6.951e-01\n",
      "Epoch 15/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.751e-01, train_iou: 7.021e-01\n",
      "Epoch 15/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.615e-01, train_iou: 7.225e-01\n",
      "Epoch 15/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 1.690e-01, train_iou: 7.115e-01\n",
      "Epoch 15/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.701e-01, train_iou: 7.099e-01\n",
      "Epoch 15/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.672e-01, train_iou: 7.141e-01\n",
      "Epoch 15/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 1.798e-01, train_iou: 6.967e-01\n",
      "Epoch 15/100 | Batch 8/63 | Took 0.65s | train_dice_loss: 1.914e-01, train_iou: 6.811e-01\n",
      "Epoch 15/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 1.916e-01, train_iou: 6.806e-01\n",
      "Epoch 15/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 1.893e-01, train_iou: 6.837e-01\n",
      "Epoch 15/100 | Batch 11/63 | Took 0.63s | train_dice_loss: 1.874e-01, train_iou: 6.862e-01\n",
      "Epoch 15/100 | Batch 12/63 | Took 0.63s | train_dice_loss: 1.866e-01, train_iou: 6.872e-01\n",
      "Epoch 15/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.833e-01, train_iou: 6.919e-01\n",
      "Epoch 15/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.833e-01, train_iou: 6.917e-01\n",
      "Epoch 15/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.819e-01, train_iou: 6.938e-01\n",
      "Epoch 15/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.824e-01, train_iou: 6.929e-01\n",
      "Epoch 15/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.805e-01, train_iou: 6.956e-01\n",
      "Epoch 15/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.795e-01, train_iou: 6.970e-01\n",
      "Epoch 15/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.798e-01, train_iou: 6.965e-01\n",
      "Epoch 15/100 | Batch 20/63 | Took 0.62s | train_dice_loss: 1.779e-01, train_iou: 6.993e-01\n",
      "Epoch 15/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.782e-01, train_iou: 6.989e-01\n",
      "Epoch 15/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.789e-01, train_iou: 6.977e-01\n",
      "Epoch 15/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 1.780e-01, train_iou: 6.990e-01\n",
      "Epoch 15/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 1.762e-01, train_iou: 7.017e-01\n",
      "Epoch 15/100 | Batch 25/63 | Took 0.68s | train_dice_loss: 1.756e-01, train_iou: 7.026e-01\n",
      "Epoch 15/100 | Batch 26/63 | Took 0.62s | train_dice_loss: 1.757e-01, train_iou: 7.024e-01\n",
      "Epoch 15/100 | Batch 27/63 | Took 0.62s | train_dice_loss: 1.767e-01, train_iou: 7.009e-01\n",
      "Epoch 15/100 | Batch 28/63 | Took 0.62s | train_dice_loss: 1.744e-01, train_iou: 7.043e-01\n",
      "Epoch 15/100 | Batch 29/63 | Took 0.62s | train_dice_loss: 1.728e-01, train_iou: 7.067e-01\n",
      "Epoch 15/100 | Batch 30/63 | Took 0.62s | train_dice_loss: 1.736e-01, train_iou: 7.056e-01\n",
      "Epoch 15/100 | Batch 31/63 | Took 0.62s | train_dice_loss: 1.726e-01, train_iou: 7.069e-01\n",
      "Epoch 15/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.741e-01, train_iou: 7.048e-01\n",
      "Epoch 15/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.022e-01\n",
      "Epoch 15/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.753e-01, train_iou: 7.031e-01\n",
      "Epoch 15/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.748e-01, train_iou: 7.039e-01\n",
      "Epoch 15/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.751e-01, train_iou: 7.034e-01\n",
      "Epoch 15/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.741e-01, train_iou: 7.048e-01\n",
      "Epoch 15/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.733e-01, train_iou: 7.060e-01\n",
      "Epoch 15/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.732e-01, train_iou: 7.061e-01\n",
      "Epoch 15/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.760e-01, train_iou: 7.023e-01\n",
      "Epoch 15/100 | Batch 41/63 | Took 0.62s | train_dice_loss: 1.753e-01, train_iou: 7.034e-01\n",
      "Epoch 15/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.743e-01, train_iou: 7.048e-01\n",
      "Epoch 15/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.738e-01, train_iou: 7.056e-01\n",
      "Epoch 15/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.736e-01, train_iou: 7.058e-01\n",
      "Epoch 15/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.730e-01, train_iou: 7.067e-01\n",
      "Epoch 15/100 | Batch 46/63 | Took 0.62s | train_dice_loss: 1.727e-01, train_iou: 7.071e-01\n",
      "Epoch 15/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.731e-01, train_iou: 7.064e-01\n",
      "Epoch 15/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.743e-01, train_iou: 7.047e-01\n",
      "Epoch 15/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.742e-01, train_iou: 7.048e-01\n",
      "Epoch 15/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.740e-01, train_iou: 7.052e-01\n",
      "Epoch 15/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.748e-01, train_iou: 7.040e-01\n",
      "Epoch 15/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.024e-01\n",
      "Epoch 15/100 | Batch 53/63 | Took 0.67s | train_dice_loss: 1.755e-01, train_iou: 7.029e-01\n",
      "Epoch 15/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.759e-01, train_iou: 7.024e-01\n",
      "Epoch 15/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.757e-01, train_iou: 7.026e-01\n",
      "Epoch 15/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.753e-01, train_iou: 7.033e-01\n",
      "Epoch 15/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.748e-01, train_iou: 7.039e-01\n",
      "Epoch 15/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.747e-01, train_iou: 7.040e-01\n",
      "Epoch 15/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.739e-01, train_iou: 7.052e-01\n",
      "Epoch 15/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.743e-01, train_iou: 7.047e-01\n",
      "Epoch 15/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.747e-01, train_iou: 7.040e-01\n",
      "Epoch 15/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.748e-01, train_iou: 7.038e-01\n",
      "Epoch 15/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.743e-01, train_iou: 7.047e-01\n",
      "Epoch 15/100 | Took 142.10s | val_dice_loss: 1.905e-01, val_iou: 6.896e-01\n",
      "====================\n",
      "Epoch 16/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.622e-01, train_iou: 7.209e-01\n",
      "Epoch 16/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.647e-01, train_iou: 7.174e-01\n",
      "Epoch 16/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.649e-01, train_iou: 7.171e-01\n",
      "Epoch 16/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 1.716e-01, train_iou: 7.075e-01\n",
      "Epoch 16/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.660e-01, train_iou: 7.156e-01\n",
      "Epoch 16/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.597e-01, train_iou: 7.251e-01\n",
      "Epoch 16/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.639e-01, train_iou: 7.190e-01\n",
      "Epoch 16/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 1.648e-01, train_iou: 7.176e-01\n",
      "Epoch 16/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 1.688e-01, train_iou: 7.119e-01\n",
      "Epoch 16/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.655e-01, train_iou: 7.167e-01\n",
      "Epoch 16/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 1.623e-01, train_iou: 7.215e-01\n",
      "Epoch 16/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 1.623e-01, train_iou: 7.214e-01\n",
      "Epoch 16/100 | Batch 13/63 | Took 0.62s | train_dice_loss: 1.625e-01, train_iou: 7.211e-01\n",
      "Epoch 16/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.627e-01, train_iou: 7.208e-01\n",
      "Epoch 16/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 1.631e-01, train_iou: 7.202e-01\n",
      "Epoch 16/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.683e-01, train_iou: 7.129e-01\n",
      "Epoch 16/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.677e-01, train_iou: 7.137e-01\n",
      "Epoch 16/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.672e-01, train_iou: 7.145e-01\n",
      "Epoch 16/100 | Batch 19/63 | Took 0.62s | train_dice_loss: 1.708e-01, train_iou: 7.094e-01\n",
      "Epoch 16/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.693e-01, train_iou: 7.116e-01\n",
      "Epoch 16/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.675e-01, train_iou: 7.143e-01\n",
      "Epoch 16/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.694e-01, train_iou: 7.115e-01\n",
      "Epoch 16/100 | Batch 23/63 | Took 0.62s | train_dice_loss: 1.685e-01, train_iou: 7.128e-01\n",
      "Epoch 16/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 1.671e-01, train_iou: 7.149e-01\n",
      "Epoch 16/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.689e-01, train_iou: 7.123e-01\n",
      "Epoch 16/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.694e-01, train_iou: 7.115e-01\n",
      "Epoch 16/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.674e-01, train_iou: 7.146e-01\n",
      "Epoch 16/100 | Batch 28/63 | Took 0.64s | train_dice_loss: 1.693e-01, train_iou: 7.119e-01\n",
      "Epoch 16/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.706e-01, train_iou: 7.100e-01\n",
      "Epoch 16/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.725e-01, train_iou: 7.072e-01\n",
      "Epoch 16/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.722e-01, train_iou: 7.077e-01\n",
      "Epoch 16/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.720e-01, train_iou: 7.079e-01\n",
      "Epoch 16/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.747e-01, train_iou: 7.042e-01\n",
      "Epoch 16/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.738e-01, train_iou: 7.055e-01\n",
      "Epoch 16/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.729e-01, train_iou: 7.068e-01\n",
      "Epoch 16/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.725e-01, train_iou: 7.074e-01\n",
      "Epoch 16/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.717e-01, train_iou: 7.085e-01\n",
      "Epoch 16/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.717e-01, train_iou: 7.085e-01\n",
      "Epoch 16/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.707e-01, train_iou: 7.098e-01\n",
      "Epoch 16/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.703e-01, train_iou: 7.105e-01\n",
      "Epoch 16/100 | Batch 41/63 | Took 0.62s | train_dice_loss: 1.695e-01, train_iou: 7.117e-01\n",
      "Epoch 16/100 | Batch 42/63 | Took 0.62s | train_dice_loss: 1.691e-01, train_iou: 7.122e-01\n",
      "Epoch 16/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.685e-01, train_iou: 7.130e-01\n",
      "Epoch 16/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.695e-01, train_iou: 7.116e-01\n",
      "Epoch 16/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.696e-01, train_iou: 7.114e-01\n",
      "Epoch 16/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.686e-01, train_iou: 7.129e-01\n",
      "Epoch 16/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.671e-01, train_iou: 7.153e-01\n",
      "Epoch 16/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.669e-01, train_iou: 7.156e-01\n",
      "Epoch 16/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.684e-01, train_iou: 7.134e-01\n",
      "Epoch 16/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.685e-01, train_iou: 7.133e-01\n",
      "Epoch 16/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.679e-01, train_iou: 7.140e-01\n",
      "Epoch 16/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.681e-01, train_iou: 7.137e-01\n",
      "Epoch 16/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.678e-01, train_iou: 7.141e-01\n",
      "Epoch 16/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.682e-01, train_iou: 7.136e-01\n",
      "Epoch 16/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.683e-01, train_iou: 7.134e-01\n",
      "Epoch 16/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.685e-01, train_iou: 7.131e-01\n",
      "Epoch 16/100 | Batch 57/63 | Took 0.67s | train_dice_loss: 1.707e-01, train_iou: 7.102e-01\n",
      "Epoch 16/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.706e-01, train_iou: 7.102e-01\n",
      "Epoch 16/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.710e-01, train_iou: 7.096e-01\n",
      "Epoch 16/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.712e-01, train_iou: 7.093e-01\n",
      "Epoch 16/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.721e-01, train_iou: 7.080e-01\n",
      "Epoch 16/100 | Batch 62/63 | Took 0.64s | train_dice_loss: 1.721e-01, train_iou: 7.080e-01\n",
      "Epoch 16/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.724e-01, train_iou: 7.076e-01\n",
      "Epoch 16/100 | Took 139.10s | val_dice_loss: 1.876e-01, val_iou: 6.938e-01\n",
      "====================\n",
      "Epoch 17/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.493e-01, train_iou: 7.404e-01\n",
      "Epoch 17/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.451e-01, train_iou: 7.468e-01\n",
      "Epoch 17/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.636e-01, train_iou: 7.199e-01\n",
      "Epoch 17/100 | Batch 4/63 | Took 0.63s | train_dice_loss: 1.598e-01, train_iou: 7.253e-01\n",
      "Epoch 17/100 | Batch 5/63 | Took 0.65s | train_dice_loss: 1.742e-01, train_iou: 7.049e-01\n",
      "Epoch 17/100 | Batch 6/63 | Took 0.63s | train_dice_loss: 1.718e-01, train_iou: 7.082e-01\n",
      "Epoch 17/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 1.671e-01, train_iou: 7.150e-01\n",
      "Epoch 17/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 1.640e-01, train_iou: 7.195e-01\n",
      "Epoch 17/100 | Batch 9/63 | Took 0.62s | train_dice_loss: 1.648e-01, train_iou: 7.182e-01\n",
      "Epoch 17/100 | Batch 10/63 | Took 0.62s | train_dice_loss: 1.704e-01, train_iou: 7.102e-01\n",
      "Epoch 17/100 | Batch 11/63 | Took 0.63s | train_dice_loss: 1.665e-01, train_iou: 7.160e-01\n",
      "Epoch 17/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 1.687e-01, train_iou: 7.127e-01\n",
      "Epoch 17/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.682e-01, train_iou: 7.133e-01\n",
      "Epoch 17/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.729e-01, train_iou: 7.067e-01\n",
      "Epoch 17/100 | Batch 15/63 | Took 0.63s | train_dice_loss: 1.743e-01, train_iou: 7.046e-01\n",
      "Epoch 17/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.728e-01, train_iou: 7.068e-01\n",
      "Epoch 17/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.745e-01, train_iou: 7.043e-01\n",
      "Epoch 17/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.752e-01, train_iou: 7.032e-01\n",
      "Epoch 17/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.741e-01, train_iou: 7.047e-01\n",
      "Epoch 17/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.741e-01, train_iou: 7.047e-01\n",
      "Epoch 17/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.753e-01, train_iou: 7.029e-01\n",
      "Epoch 17/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.735e-01, train_iou: 7.056e-01\n",
      "Epoch 17/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.714e-01, train_iou: 7.087e-01\n",
      "Epoch 17/100 | Batch 24/63 | Took 0.63s | train_dice_loss: 1.720e-01, train_iou: 7.077e-01\n",
      "Epoch 17/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.738e-01, train_iou: 7.052e-01\n",
      "Epoch 17/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.738e-01, train_iou: 7.052e-01\n",
      "Epoch 17/100 | Batch 27/63 | Took 0.62s | train_dice_loss: 1.728e-01, train_iou: 7.066e-01\n",
      "Epoch 17/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 1.727e-01, train_iou: 7.067e-01\n",
      "Epoch 17/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.731e-01, train_iou: 7.061e-01\n",
      "Epoch 17/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.735e-01, train_iou: 7.055e-01\n",
      "Epoch 17/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.717e-01, train_iou: 7.081e-01\n",
      "Epoch 17/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.699e-01, train_iou: 7.109e-01\n",
      "Epoch 17/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.684e-01, train_iou: 7.131e-01\n",
      "Epoch 17/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.694e-01, train_iou: 7.117e-01\n",
      "Epoch 17/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.687e-01, train_iou: 7.127e-01\n",
      "Epoch 17/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.684e-01, train_iou: 7.130e-01\n",
      "Epoch 17/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.692e-01, train_iou: 7.119e-01\n",
      "Epoch 17/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.706e-01, train_iou: 7.099e-01\n",
      "Epoch 17/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.707e-01, train_iou: 7.097e-01\n",
      "Epoch 17/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.700e-01, train_iou: 7.108e-01\n",
      "Epoch 17/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.686e-01, train_iou: 7.129e-01\n",
      "Epoch 17/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.682e-01, train_iou: 7.134e-01\n",
      "Epoch 17/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.691e-01, train_iou: 7.121e-01\n",
      "Epoch 17/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.694e-01, train_iou: 7.116e-01\n",
      "Epoch 17/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.707e-01, train_iou: 7.099e-01\n",
      "Epoch 17/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.697e-01, train_iou: 7.114e-01\n",
      "Epoch 17/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.694e-01, train_iou: 7.117e-01\n",
      "Epoch 17/100 | Batch 48/63 | Took 0.62s | train_dice_loss: 1.702e-01, train_iou: 7.106e-01\n",
      "Epoch 17/100 | Batch 49/63 | Took 0.62s | train_dice_loss: 1.692e-01, train_iou: 7.120e-01\n",
      "Epoch 17/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.700e-01, train_iou: 7.109e-01\n",
      "Epoch 17/100 | Batch 51/63 | Took 0.66s | train_dice_loss: 1.691e-01, train_iou: 7.122e-01\n",
      "Epoch 17/100 | Batch 52/63 | Took 0.62s | train_dice_loss: 1.690e-01, train_iou: 7.124e-01\n",
      "Epoch 17/100 | Batch 53/63 | Took 0.62s | train_dice_loss: 1.690e-01, train_iou: 7.123e-01\n",
      "Epoch 17/100 | Batch 54/63 | Took 0.62s | train_dice_loss: 1.689e-01, train_iou: 7.125e-01\n",
      "Epoch 17/100 | Batch 55/63 | Took 0.62s | train_dice_loss: 1.693e-01, train_iou: 7.118e-01\n",
      "Epoch 17/100 | Batch 56/63 | Took 0.62s | train_dice_loss: 1.699e-01, train_iou: 7.109e-01\n",
      "Epoch 17/100 | Batch 57/63 | Took 0.62s | train_dice_loss: 1.691e-01, train_iou: 7.122e-01\n",
      "Epoch 17/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.685e-01, train_iou: 7.131e-01\n",
      "Epoch 17/100 | Batch 59/63 | Took 0.62s | train_dice_loss: 1.689e-01, train_iou: 7.124e-01\n",
      "Epoch 17/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.686e-01, train_iou: 7.129e-01\n",
      "Epoch 17/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.683e-01, train_iou: 7.133e-01\n",
      "Epoch 17/100 | Batch 62/63 | Took 0.62s | train_dice_loss: 1.683e-01, train_iou: 7.133e-01\n",
      "Epoch 17/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.678e-01, train_iou: 7.141e-01\n",
      "Epoch 17/100 | Took 137.04s | val_dice_loss: 1.906e-01, val_iou: 6.890e-01\n",
      "====================\n",
      "Epoch 18/100 | Batch 1/63 | Took 0.64s | train_dice_loss: 2.005e-01, train_iou: 6.660e-01\n",
      "Epoch 18/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.823e-01, train_iou: 6.919e-01\n",
      "Epoch 18/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.717e-01, train_iou: 7.075e-01\n",
      "Epoch 18/100 | Batch 4/63 | Took 0.63s | train_dice_loss: 1.685e-01, train_iou: 7.120e-01\n",
      "Epoch 18/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.685e-01, train_iou: 7.120e-01\n",
      "Epoch 18/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.697e-01, train_iou: 7.101e-01\n",
      "Epoch 18/100 | Batch 7/63 | Took 0.62s | train_dice_loss: 1.647e-01, train_iou: 7.177e-01\n",
      "Epoch 18/100 | Batch 8/63 | Took 0.63s | train_dice_loss: 1.671e-01, train_iou: 7.141e-01\n",
      "Epoch 18/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 1.657e-01, train_iou: 7.162e-01\n",
      "Epoch 18/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.602e-01, train_iou: 7.246e-01\n",
      "Epoch 18/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 1.630e-01, train_iou: 7.205e-01\n",
      "Epoch 18/100 | Batch 12/63 | Took 0.62s | train_dice_loss: 1.634e-01, train_iou: 7.198e-01\n",
      "Epoch 18/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.594e-01, train_iou: 7.260e-01\n",
      "Epoch 18/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.595e-01, train_iou: 7.258e-01\n",
      "Epoch 18/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 1.589e-01, train_iou: 7.265e-01\n",
      "Epoch 18/100 | Batch 16/63 | Took 0.63s | train_dice_loss: 1.597e-01, train_iou: 7.253e-01\n",
      "Epoch 18/100 | Batch 17/63 | Took 0.63s | train_dice_loss: 1.609e-01, train_iou: 7.236e-01\n",
      "Epoch 18/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.635e-01, train_iou: 7.198e-01\n",
      "Epoch 18/100 | Batch 19/63 | Took 0.63s | train_dice_loss: 1.613e-01, train_iou: 7.232e-01\n",
      "Epoch 18/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.641e-01, train_iou: 7.191e-01\n",
      "Epoch 18/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.635e-01, train_iou: 7.199e-01\n",
      "Epoch 18/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.645e-01, train_iou: 7.186e-01\n",
      "Epoch 18/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.661e-01, train_iou: 7.162e-01\n",
      "Epoch 18/100 | Batch 24/63 | Took 0.66s | train_dice_loss: 1.653e-01, train_iou: 7.173e-01\n",
      "Epoch 18/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.654e-01, train_iou: 7.171e-01\n",
      "Epoch 18/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.667e-01, train_iou: 7.153e-01\n",
      "Epoch 18/100 | Batch 27/63 | Took 0.68s | train_dice_loss: 1.716e-01, train_iou: 7.088e-01\n",
      "Epoch 18/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 1.709e-01, train_iou: 7.097e-01\n",
      "Epoch 18/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.692e-01, train_iou: 7.122e-01\n",
      "Epoch 18/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.691e-01, train_iou: 7.123e-01\n",
      "Epoch 18/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.684e-01, train_iou: 7.133e-01\n",
      "Epoch 18/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.690e-01, train_iou: 7.124e-01\n",
      "Epoch 18/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.688e-01, train_iou: 7.126e-01\n",
      "Epoch 18/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.689e-01, train_iou: 7.125e-01\n",
      "Epoch 18/100 | Batch 35/63 | Took 0.63s | train_dice_loss: 1.706e-01, train_iou: 7.101e-01\n",
      "Epoch 18/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.716e-01, train_iou: 7.086e-01\n",
      "Epoch 18/100 | Batch 37/63 | Took 0.67s | train_dice_loss: 1.708e-01, train_iou: 7.098e-01\n",
      "Epoch 18/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.715e-01, train_iou: 7.087e-01\n",
      "Epoch 18/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.727e-01, train_iou: 7.070e-01\n",
      "Epoch 18/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.726e-01, train_iou: 7.072e-01\n",
      "Epoch 18/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.725e-01, train_iou: 7.073e-01\n",
      "Epoch 18/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.737e-01, train_iou: 7.055e-01\n",
      "Epoch 18/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.728e-01, train_iou: 7.068e-01\n",
      "Epoch 18/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.730e-01, train_iou: 7.066e-01\n",
      "Epoch 18/100 | Batch 45/63 | Took 0.63s | train_dice_loss: 1.738e-01, train_iou: 7.053e-01\n",
      "Epoch 18/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.738e-01, train_iou: 7.053e-01\n",
      "Epoch 18/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.731e-01, train_iou: 7.063e-01\n",
      "Epoch 18/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.742e-01, train_iou: 7.048e-01\n",
      "Epoch 18/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.737e-01, train_iou: 7.056e-01\n",
      "Epoch 18/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.740e-01, train_iou: 7.051e-01\n",
      "Epoch 18/100 | Batch 51/63 | Took 0.63s | train_dice_loss: 1.737e-01, train_iou: 7.055e-01\n",
      "Epoch 18/100 | Batch 52/63 | Took 0.63s | train_dice_loss: 1.728e-01, train_iou: 7.069e-01\n",
      "Epoch 18/100 | Batch 53/63 | Took 0.63s | train_dice_loss: 1.729e-01, train_iou: 7.066e-01\n",
      "Epoch 18/100 | Batch 54/63 | Took 0.63s | train_dice_loss: 1.731e-01, train_iou: 7.063e-01\n",
      "Epoch 18/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.733e-01, train_iou: 7.060e-01\n",
      "Epoch 18/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.730e-01, train_iou: 7.064e-01\n",
      "Epoch 18/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.727e-01, train_iou: 7.068e-01\n",
      "Epoch 18/100 | Batch 58/63 | Took 0.63s | train_dice_loss: 1.725e-01, train_iou: 7.071e-01\n",
      "Epoch 18/100 | Batch 59/63 | Took 0.62s | train_dice_loss: 1.719e-01, train_iou: 7.080e-01\n",
      "Epoch 18/100 | Batch 60/63 | Took 0.62s | train_dice_loss: 1.712e-01, train_iou: 7.091e-01\n",
      "Epoch 18/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.706e-01, train_iou: 7.099e-01\n",
      "Epoch 18/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.702e-01, train_iou: 7.105e-01\n",
      "Epoch 18/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.706e-01, train_iou: 7.099e-01\n",
      "Epoch 18/100 | Took 137.90s | val_dice_loss: 1.927e-01, val_iou: 6.877e-01\n",
      "====================\n",
      "Epoch 19/100 | Batch 1/63 | Took 0.63s | train_dice_loss: 1.574e-01, train_iou: 7.281e-01\n",
      "Epoch 19/100 | Batch 2/63 | Took 0.62s | train_dice_loss: 1.433e-01, train_iou: 7.497e-01\n",
      "Epoch 19/100 | Batch 3/63 | Took 0.62s | train_dice_loss: 1.797e-01, train_iou: 6.987e-01\n",
      "Epoch 19/100 | Batch 4/63 | Took 0.62s | train_dice_loss: 1.655e-01, train_iou: 7.193e-01\n",
      "Epoch 19/100 | Batch 5/63 | Took 0.62s | train_dice_loss: 1.640e-01, train_iou: 7.209e-01\n",
      "Epoch 19/100 | Batch 6/63 | Took 0.62s | train_dice_loss: 1.666e-01, train_iou: 7.167e-01\n",
      "Epoch 19/100 | Batch 7/63 | Took 0.63s | train_dice_loss: 1.665e-01, train_iou: 7.165e-01\n",
      "Epoch 19/100 | Batch 8/63 | Took 0.62s | train_dice_loss: 1.649e-01, train_iou: 7.186e-01\n",
      "Epoch 19/100 | Batch 9/63 | Took 0.63s | train_dice_loss: 1.615e-01, train_iou: 7.236e-01\n",
      "Epoch 19/100 | Batch 10/63 | Took 0.63s | train_dice_loss: 1.626e-01, train_iou: 7.219e-01\n",
      "Epoch 19/100 | Batch 11/63 | Took 0.62s | train_dice_loss: 1.638e-01, train_iou: 7.199e-01\n",
      "Epoch 19/100 | Batch 12/63 | Took 0.63s | train_dice_loss: 1.707e-01, train_iou: 7.104e-01\n",
      "Epoch 19/100 | Batch 13/63 | Took 0.63s | train_dice_loss: 1.681e-01, train_iou: 7.142e-01\n",
      "Epoch 19/100 | Batch 14/63 | Took 0.63s | train_dice_loss: 1.667e-01, train_iou: 7.160e-01\n",
      "Epoch 19/100 | Batch 15/63 | Took 0.62s | train_dice_loss: 1.664e-01, train_iou: 7.164e-01\n",
      "Epoch 19/100 | Batch 16/63 | Took 0.62s | train_dice_loss: 1.649e-01, train_iou: 7.186e-01\n",
      "Epoch 19/100 | Batch 17/63 | Took 0.62s | train_dice_loss: 1.678e-01, train_iou: 7.143e-01\n",
      "Epoch 19/100 | Batch 18/63 | Took 0.63s | train_dice_loss: 1.678e-01, train_iou: 7.142e-01\n",
      "Epoch 19/100 | Batch 19/63 | Took 0.62s | train_dice_loss: 1.678e-01, train_iou: 7.142e-01\n",
      "Epoch 19/100 | Batch 20/63 | Took 0.63s | train_dice_loss: 1.692e-01, train_iou: 7.121e-01\n",
      "Epoch 19/100 | Batch 21/63 | Took 0.63s | train_dice_loss: 1.673e-01, train_iou: 7.149e-01\n",
      "Epoch 19/100 | Batch 22/63 | Took 0.63s | train_dice_loss: 1.670e-01, train_iou: 7.152e-01\n",
      "Epoch 19/100 | Batch 23/63 | Took 0.63s | train_dice_loss: 1.677e-01, train_iou: 7.142e-01\n",
      "Epoch 19/100 | Batch 24/63 | Took 0.62s | train_dice_loss: 1.679e-01, train_iou: 7.138e-01\n",
      "Epoch 19/100 | Batch 25/63 | Took 0.63s | train_dice_loss: 1.667e-01, train_iou: 7.155e-01\n",
      "Epoch 19/100 | Batch 26/63 | Took 0.63s | train_dice_loss: 1.662e-01, train_iou: 7.162e-01\n",
      "Epoch 19/100 | Batch 27/63 | Took 0.63s | train_dice_loss: 1.650e-01, train_iou: 7.181e-01\n",
      "Epoch 19/100 | Batch 28/63 | Took 0.63s | train_dice_loss: 1.636e-01, train_iou: 7.202e-01\n",
      "Epoch 19/100 | Batch 29/63 | Took 0.63s | train_dice_loss: 1.651e-01, train_iou: 7.180e-01\n",
      "Epoch 19/100 | Batch 30/63 | Took 0.63s | train_dice_loss: 1.652e-01, train_iou: 7.177e-01\n",
      "Epoch 19/100 | Batch 31/63 | Took 0.63s | train_dice_loss: 1.645e-01, train_iou: 7.188e-01\n",
      "Epoch 19/100 | Batch 32/63 | Took 0.63s | train_dice_loss: 1.628e-01, train_iou: 7.214e-01\n",
      "Epoch 19/100 | Batch 33/63 | Took 0.63s | train_dice_loss: 1.638e-01, train_iou: 7.200e-01\n",
      "Epoch 19/100 | Batch 34/63 | Took 0.63s | train_dice_loss: 1.632e-01, train_iou: 7.207e-01\n",
      "Epoch 19/100 | Batch 35/63 | Took 0.62s | train_dice_loss: 1.620e-01, train_iou: 7.225e-01\n",
      "Epoch 19/100 | Batch 36/63 | Took 0.63s | train_dice_loss: 1.624e-01, train_iou: 7.220e-01\n",
      "Epoch 19/100 | Batch 37/63 | Took 0.63s | train_dice_loss: 1.619e-01, train_iou: 7.226e-01\n",
      "Epoch 19/100 | Batch 38/63 | Took 0.63s | train_dice_loss: 1.629e-01, train_iou: 7.212e-01\n",
      "Epoch 19/100 | Batch 39/63 | Took 0.63s | train_dice_loss: 1.629e-01, train_iou: 7.212e-01\n",
      "Epoch 19/100 | Batch 40/63 | Took 0.63s | train_dice_loss: 1.641e-01, train_iou: 7.194e-01\n",
      "Epoch 19/100 | Batch 41/63 | Took 0.63s | train_dice_loss: 1.653e-01, train_iou: 7.177e-01\n",
      "Epoch 19/100 | Batch 42/63 | Took 0.63s | train_dice_loss: 1.661e-01, train_iou: 7.164e-01\n",
      "Epoch 19/100 | Batch 43/63 | Took 0.63s | train_dice_loss: 1.654e-01, train_iou: 7.176e-01\n",
      "Epoch 19/100 | Batch 44/63 | Took 0.63s | train_dice_loss: 1.660e-01, train_iou: 7.167e-01\n",
      "Epoch 19/100 | Batch 45/63 | Took 0.68s | train_dice_loss: 1.656e-01, train_iou: 7.171e-01\n",
      "Epoch 19/100 | Batch 46/63 | Took 0.63s | train_dice_loss: 1.649e-01, train_iou: 7.182e-01\n",
      "Epoch 19/100 | Batch 47/63 | Took 0.63s | train_dice_loss: 1.661e-01, train_iou: 7.166e-01\n",
      "Epoch 19/100 | Batch 48/63 | Took 0.63s | train_dice_loss: 1.656e-01, train_iou: 7.172e-01\n",
      "Epoch 19/100 | Batch 49/63 | Took 0.63s | train_dice_loss: 1.649e-01, train_iou: 7.183e-01\n",
      "Epoch 19/100 | Batch 50/63 | Took 0.63s | train_dice_loss: 1.660e-01, train_iou: 7.167e-01\n",
      "Epoch 19/100 | Batch 51/63 | Took 0.62s | train_dice_loss: 1.661e-01, train_iou: 7.166e-01\n",
      "Epoch 19/100 | Batch 52/63 | Took 0.62s | train_dice_loss: 1.667e-01, train_iou: 7.156e-01\n",
      "Epoch 19/100 | Batch 53/63 | Took 0.62s | train_dice_loss: 1.667e-01, train_iou: 7.157e-01\n",
      "Epoch 19/100 | Batch 54/63 | Took 0.62s | train_dice_loss: 1.668e-01, train_iou: 7.155e-01\n",
      "Epoch 19/100 | Batch 55/63 | Took 0.63s | train_dice_loss: 1.667e-01, train_iou: 7.156e-01\n",
      "Epoch 19/100 | Batch 56/63 | Took 0.63s | train_dice_loss: 1.668e-01, train_iou: 7.154e-01\n",
      "Epoch 19/100 | Batch 57/63 | Took 0.63s | train_dice_loss: 1.668e-01, train_iou: 7.153e-01\n",
      "Epoch 19/100 | Batch 58/63 | Took 0.66s | train_dice_loss: 1.657e-01, train_iou: 7.170e-01\n",
      "Epoch 19/100 | Batch 59/63 | Took 0.63s | train_dice_loss: 1.671e-01, train_iou: 7.151e-01\n",
      "Epoch 19/100 | Batch 60/63 | Took 0.63s | train_dice_loss: 1.663e-01, train_iou: 7.163e-01\n",
      "Epoch 19/100 | Batch 61/63 | Took 0.63s | train_dice_loss: 1.665e-01, train_iou: 7.160e-01\n",
      "Epoch 19/100 | Batch 62/63 | Took 0.63s | train_dice_loss: 1.658e-01, train_iou: 7.170e-01\n",
      "Epoch 19/100 | Batch 63/63 | Took 0.32s | train_dice_loss: 1.651e-01, train_iou: 7.181e-01\n",
      "Epoch 19/100 | Took 138.00s | val_dice_loss: 1.876e-01, val_iou: 6.937e-01\n",
      "====================\n",
      "Early Stopped\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(params=net.parameters(), lr=0.001)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=net, optimizer=optimizer,\n",
    "    train_dataset=train_dataset, val_dataset=val_dataset,\n",
    "    train_batch_size=16, val_batch_size=4,\n",
    "    device=device,\n",
    ")\n",
    "trainer.train(\n",
    "    n_epochs=100, patience=5,\n",
    "    tolerance=0., checkpoint_path='./checkpoints/',\n",
    "    save_frequency=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3.Evaluate your model using the test images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da9342d",
   "metadata": {},
   "source": [
    "We need a plotting function to plot the input images, the groundtruth masks, and predicted masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd80c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "951d3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    images: torch.Tensor,\n",
    "    groundtruths: torch.Tensor,\n",
    "    predictions: torch.Tensor,\n",
    "    notes: List[str],\n",
    ") -> None:\n",
    "\n",
    "    assert groundtruths.shape == predictions.shape\n",
    "    assert groundtruths.ndim == 4   # (batch_size, n_channels, height, width)\n",
    "    assert groundtruths.shape[1] == 1, 'Expect n_channels to be 1'\n",
    "    assert notes is None or len(notes) == groundtruths.shape[0]\n",
    "\n",
    "    os.makedirs(f\"./results\", exist_ok=True)\n",
    "\n",
    "    images = images.to(device=torch.device('cpu'))\n",
    "    groundtruths = groundtruths.to(device=torch.device('cpu'))\n",
    "    predictions = predictions.to(device=torch.device('cpu'))\n",
    "\n",
    "    # Ensure that the plot respect the tensor's shape\n",
    "\n",
    "    for idx in range(predictions.shape[0]):\n",
    "        image: torch.Tensor = images[idx]\n",
    "        groundtruth: torch.Tensor = groundtruths[idx]\n",
    "        prediction: torch.Tensor = predictions[idx]\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        axs[0].imshow(\n",
    "            image.squeeze(dim=0),\n",
    "            cmap='gray',\n",
    "        )\n",
    "        axs[0].set_xticks([])\n",
    "        axs[0].set_yticks([])\n",
    "        axs[0].set_title(f'$image$', fontsize=20)\n",
    "        axs[1].imshow(\n",
    "            groundtruth.squeeze(dim=0),\n",
    "            cmap='gray',\n",
    "        )\n",
    "        axs[1].set_xticks([])\n",
    "        axs[1].set_yticks([])\n",
    "        axs[1].set_title(f'$groundtruth$', fontsize=20)\n",
    "        prediction: torch.Tensor = (torch.sigmoid(input=prediction) > 0.5).int()\n",
    "        axs[2].imshow(\n",
    "            prediction.squeeze(dim=0),\n",
    "            cmap='gray',\n",
    "        )\n",
    "        axs[2].set_xticks([])\n",
    "        axs[2].set_yticks([])\n",
    "        axs[2].set_title(f'$prediction - {notes[idx]}$', fontsize=20)\n",
    "        fig.subplots_adjust(hspace=0.1)\n",
    "        fig.tight_layout()\n",
    "        timestamp: dt.datetime = dt.datetime.now()\n",
    "        fig.savefig(\n",
    "            f\"./results/{timestamp.strftime('%Y%m%d%H%M%S')}\"\n",
    "            f\"{timestamp.microsecond // 1000:03d}.png\"\n",
    "        )\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a97cec",
   "metadata": {},
   "source": [
    "We need a `Predictor` class to make predictions on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a3cef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: torch.device) -> None:\n",
    "        self.model: nn.Module = model.to(device=device)\n",
    "        self.device: torch.device = device\n",
    "        self.loss_function: nn.Module = SoftDiceLoss()\n",
    "        self.evaluation_metric: nn.Module = IOU()\n",
    "\n",
    "    def predict(self, dataset: BirdSoundDataset) -> float:\n",
    "        self.model.eval()\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=False) # sample-level method, not batch-level\n",
    "\n",
    "        batch_images: List[torch.Tensor] = []\n",
    "        batch_groundtruths: List[torch.Tensor] = []\n",
    "        batch_predictions: List[torch.Tensor] = []\n",
    "        iou_values: List[float] = []\n",
    "        metric_notes: List[str] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Loop through each batch\n",
    "            for batch_image, batch_groundtruth in dataloader:\n",
    "                assert batch_image.ndim == 4\n",
    "                batch_image: torch.Tensor = batch_image.to(device=self.device)\n",
    "                batch_groundtruth: torch.Tensor = batch_groundtruth.to(device=self.device)\n",
    "                batch_prediction: torch.Tensor = self.model(input=batch_image)\n",
    "                assert batch_prediction.shape == batch_groundtruth.shape\n",
    "\n",
    "                dice_loss = self.loss_function(\n",
    "                    logits=batch_prediction, groundtruths=batch_groundtruth,\n",
    "                ).item()\n",
    "                iou = self.evaluation_metric(\n",
    "                    logits=batch_prediction, groundtruths=batch_groundtruth,\n",
    "                ).item()\n",
    "                batch_images.append(batch_image)\n",
    "                batch_groundtruths.append(batch_groundtruth)\n",
    "                batch_predictions.append(batch_prediction)\n",
    "                iou_values.append(iou)\n",
    "                metric_notes.append(f'Dice Loss: {dice_loss:.4f}, IoU: {iou:.4f}')\n",
    "\n",
    "            images = torch.cat(tensors=batch_images, dim=0)\n",
    "            predictions = torch.cat(tensors=batch_predictions, dim=0)\n",
    "            groundtruths = torch.cat(tensors=batch_groundtruths, dim=0)\n",
    "            assert predictions.shape == groundtruths.shape\n",
    "            # Plot the prediction\n",
    "            plot_predictions(\n",
    "                images=images,\n",
    "                groundtruths=groundtruths, \n",
    "                predictions=predictions, \n",
    "                notes=metric_notes, \n",
    "            )\n",
    "\n",
    "        return sum(iou_values) / len(iou_values) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2152d",
   "metadata": {},
   "source": [
    "We have everything ready. Now, we make predictions on test dataset. Since the training process early stopped at `epoch 19`, we will load the checkpoint at this epoch to do the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94e3e87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.19253231709203\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = CheckpointLoader(r'checkpoints/epoch19.pt').load(scope=globals())\n",
    "predictor = Predictor(model=model, device=device)\n",
    "test_iou: float = predictor.predict(dataset=test_dataset)\n",
    "print(test_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Your IoU score should be higher than 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3b646",
   "metadata": {},
   "source": [
    "As can be seen, our model achieved the IoU of `67.19%` on the test dataset. Let's look at several specific predictions:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_three/master/results/20240802201944718.png\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_three/master/results/20240802201941657.png\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_three/master/results/20240802201949954.png\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_three/master/results/20240802202036598.png\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_three/master/results/20240802202052500.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 5. Write a 3-page report using LaTex and upload your paper to ResearchGate or Arxiv, and put your paper link here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13831d7a",
   "metadata": {},
   "source": [
    "Source code: https://github.com/hiepdang-ml/dnn_project_three\n",
    "\n",
    "Paper link: \n",
    "\n",
    "Model Weight: https://drive.google.com/drive/folders/1U02WWSGn-dKJ7MBtXNb9FLBhy9fJjcj2?usp=share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ac291",
   "metadata": {},
   "source": [
    "# 6. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file, and add paper link)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points     \n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2fb73",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
